{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXhsb6rm1_y6"
      },
      "source": [
        "Implementacion del algoritmo Singular value decomposition (SVD) para realizar predicciones de un sistema de recomendacion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zHkKWDBBZjck",
        "outputId": "628707d6-32a9-41ac-a92a-2aa2249eb218"
      },
      "outputs": [],
      "source": [
        "import surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "whMzg5Nl2kU6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix,lil_matrix\n",
        "\n",
        "RANDOM_STATE = 46\n",
        "train = pd.read_csv('./data/train.csv')\n",
        "test = pd.read_csv('./data/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como modelo usaremos el SVDpp de la libreria del surprise, este modelo se basa en el SVD clasico, pero añade dos mejoras significativas:\n",
        "\n",
        "Considera los ítems con los que el usuario ha interactuado, incluso si no los ha calificado.\n",
        "\n",
        "Permite mejores predicciones al capturar datos indirectos sobre las preferencias de los usuarios.\n",
        "\n",
        "Sin embargo esta implementacion aumenta el coste computacional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dCHMiJhq_Nq7",
        "outputId": "bca6ac3e-a286-4f00-a483-f4616b66363f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training SVD++ model...\n",
            "Performing hyperparameter tuning...\n",
            "Best parameters: {'n_factors': 20, 'n_epochs': 25, 'lr_all': 0.007, 'reg_all': 0.1}\n",
            " processing epoch 0\n",
            " processing epoch 1\n",
            " processing epoch 2\n",
            " processing epoch 3\n",
            " processing epoch 4\n",
            " processing epoch 5\n",
            " processing epoch 6\n",
            " processing epoch 7\n",
            " processing epoch 8\n",
            " processing epoch 9\n",
            " processing epoch 10\n",
            " processing epoch 11\n",
            " processing epoch 12\n",
            " processing epoch 13\n",
            " processing epoch 14\n",
            " processing epoch 15\n",
            " processing epoch 16\n",
            " processing epoch 17\n",
            " processing epoch 18\n",
            " processing epoch 19\n",
            " processing epoch 20\n",
            " processing epoch 21\n",
            " processing epoch 22\n",
            " processing epoch 23\n",
            " processing epoch 24\n",
            "RMSE: 1.6449\n",
            "Test RMSE: 1.6449\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVDpp\n",
        "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from surprise.accuracy import rmse\n",
        "\n",
        "# Step 1: Load and prepare the dataset\n",
        "# Assuming your data is in a CSV file with columns 'user', 'item', 'rating'\n",
        "def load_data(file_path):\n",
        "    # Load the data\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Define the format of the data\n",
        "    reader = Reader(rating_scale=(df['rating'].min(), df['rating'].max()))\n",
        "\n",
        "    # Load the data into the Surprise format\n",
        "    data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Step 2: Train SVD++ model with hyperparameter tuning\n",
        "def train_svdpp_model(data):\n",
        "    # Split data into train and test sets\n",
        "    trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_factors': [20, 30, 40],\n",
        "        'n_epochs': [20,25],\n",
        "        'lr_all': [0.007, 0.01],\n",
        "        'reg_all': [0.02,0.1]\n",
        "    \n",
        "    }\n",
        "\n",
        "    # Perform grid search\n",
        "    gs = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae'], cv=2)\n",
        "    gs.fit(data)\n",
        "\n",
        "    # Get the best parameters\n",
        "    best_params = gs.best_params['rmse']\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "    # Train with best parameters\n",
        "    algo = SVDpp(\n",
        "        n_factors=best_params['n_factors'],\n",
        "        n_epochs=best_params['n_epochs'],\n",
        "        lr_all=best_params['lr_all'],\n",
        "        reg_all=best_params['reg_all'],\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    # Test the model\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    # Calculate and print RMSE\n",
        "    test_rmse = rmse(predictions)\n",
        "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "    return algo, predictions\n",
        "\n",
        "\n",
        "# Main function to run the whole process\n",
        "def main(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    print(\"Training SVD++ model...\")\n",
        "    algo, _ = train_svdpp_model(data)\n",
        "\n",
        "    return algo\n",
        "\n",
        "# Usage example\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = \"./data/train.csv\"\n",
        "\n",
        "# Set to False if you want to skip hyperparameter tuning (faster)\n",
        "algo = main(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se ha realizado un grid search con este algoritmo, obteniendo los mejores hiperparametros de k = 20, 25 iteraciones, learning rate de 0.07 y regularizacion de 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediccion normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "    \n",
        "# Iterate through each row in the test data\n",
        "for _, row in test.iterrows():\n",
        "    user_id = row['user']\n",
        "    item_id = row['item']\n",
        "    id_value = row['ID']\n",
        "    \n",
        "    # Predict rating for this user-item pair\n",
        "    pred = algo.predict(uid=user_id, iid=item_id)\n",
        "    \n",
        "    # Store the prediction\n",
        "    predictions.append({\n",
        "        'ID': id_value,\n",
        "        'rating': pred.est\n",
        "    })\n",
        "\n",
        "# Create DataFrame from predictions\n",
        "result_df = pd.DataFrame(predictions)\n",
        "\n",
        "result_df.to_csv('./SVDpp_20k_25iter_lr007.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se ha realizado el test con kaggle, obteniendo un resultado de 1.270.\n",
        "\n",
        "Sin embargo esto unicamente se ha realizado con una prediccion unicamente del modelo, sin tener en cuenta la falta de usuarios o items en train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediccion sustitucion media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "    \n",
        "# Iterate through each row in the test data\n",
        "\n",
        "user_train_set = set(train['user'].unique())\n",
        "item_train_set = set(train['item'].unique())\n",
        "\n",
        "for _, row in test.iterrows():\n",
        "\n",
        "    user_id = row['user']\n",
        "    item_id = row['item']\n",
        "    id_value = row['ID']\n",
        "\n",
        "    if user_id not in user_train_set and item_id not in item_train_set:\n",
        "        predicted_value = train['rating'].mean()\n",
        "    elif user_id not in user_train_set:\n",
        "        predicted_value = train[train['item'] == row['item']]['rating'].mean()\n",
        "    elif item_id not in item_train_set:\n",
        "        predicted_value = train[train['user'] == row['user']]['rating'].mean()\n",
        "    else:\n",
        "        predicted_value = algo.predict(uid=user_id, iid=item_id).est\n",
        "        \n",
        "    # Store the prediction\n",
        "    predictions.append({\n",
        "        'ID': id_value,\n",
        "        'rating': predicted_value\n",
        "    })\n",
        "\n",
        "# Create DataFrame from predictions\n",
        "result_df = pd.DataFrame(predictions)\n",
        "\n",
        "result_df.to_csv('./SVDpp_20k_25iter_lr007mean_unk.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sustituyendo las predicciones de usuarios e items faltantes por la media, se obtienen resultados de 1.262."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que las notas son valores enteros, probamos a redondear los valores para que sean enteros, ya que los valores finales son enteros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "    \n",
        "# Iterate through each row in the test data\n",
        "\n",
        "user_train_set = set(train['user'].unique())\n",
        "item_train_set = set(train['item'].unique())\n",
        "\n",
        "for _, row in test.iterrows():\n",
        "\n",
        "    user_id = row['user']\n",
        "    item_id = row['item']\n",
        "    id_value = row['ID']\n",
        "\n",
        "    if user_id not in user_train_set and item_id not in item_train_set:\n",
        "        predicted_value = train['rating'].mean()\n",
        "    elif user_id not in user_train_set:\n",
        "        predicted_value = train[train['item'] == row['item']]['rating'].mean()\n",
        "    elif item_id not in item_train_set:\n",
        "        predicted_value = train[train['user'] == row['user']]['rating'].mean()\n",
        "    else:\n",
        "        predicted_value = algo.predict(uid=user_id, iid=item_id).est\n",
        "        \n",
        "    predicted_value = round(predicted_value)\n",
        "    # Store the prediction\n",
        "    predictions.append({\n",
        "        'ID': id_value,\n",
        "        'rating': predicted_value\n",
        "    })\n",
        "\n",
        "# Create DataFrame from predictions\n",
        "result_df = pd.DataFrame(predictions)\n",
        "\n",
        "result_df.to_csv('./SVDpp_20k_25iter_lr007_round_mean_unk.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este nuevo sistema de predicciones en base al redondeo mejora los resultados anteriores, obteniendo unos resultados en test de 1.237."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos las predicciones con un redondeo mas ameno. En vez de redondear siempre, solo se redondea solo si se esta seguro de que la prediccion es correcta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_round(number: float, min_round = 0.3, max_ronund = 0.7) -> float:\n",
        "    integer_part = int(number)\n",
        "    decimal_part = number - integer_part\n",
        "    \n",
        "    if decimal_part < 0.3:\n",
        "        return float(integer_part)  # Round down\n",
        "    elif decimal_part > 0.7:\n",
        "        return float(integer_part + 1)  # Round up\n",
        "    else:\n",
        "        return number\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predictions = []\n",
        "    \n",
        "# Iterate through each row in the test data\n",
        "\n",
        "user_train_set = set(train['user'].unique())\n",
        "item_train_set = set(train['item'].unique())\n",
        "\n",
        "for _, row in test.iterrows():\n",
        "\n",
        "    user_id = row['user']\n",
        "    item_id = row['item']\n",
        "    id_value = row['ID']\n",
        "\n",
        "    if user_id not in user_train_set and item_id not in item_train_set:\n",
        "        predicted_value = train['rating'].mean()\n",
        "    elif user_id not in user_train_set:\n",
        "        predicted_value = train[train['item'] == row['item']]['rating'].mean()\n",
        "    elif item_id not in item_train_set:\n",
        "        predicted_value = train[train['user'] == row['user']]['rating'].mean()\n",
        "    else:\n",
        "        predicted_value = algo.predict(uid=user_id, iid=item_id).est\n",
        "        \n",
        "    predicted_value = custom_round(predicted_value)\n",
        "    # Store the prediction\n",
        "    predictions.append({\n",
        "        'ID': id_value,\n",
        "        'rating': predicted_value\n",
        "    })\n",
        "\n",
        "# Create DataFrame from predictions\n",
        "result_df = pd.DataFrame(predictions)\n",
        "\n",
        "result_df.to_csv('./SVDpp_20k_25iter_lr007_custom_round_mean_unk.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este nuevo redondeo aunque parece ser mas seguro, empeora los resultados, esto es debido a que aunque el modelo no este seguro de si una prediccion es un valor u otro, si se decanta por un valor, seguramente sea porque el valor entero mas cercano es la prediccion verdadera."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "main_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
