{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25715</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25716</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25851</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>25923</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>25924</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user   item  rating\n",
       "0     1  25715     7.0\n",
       "1     1  25716    10.0\n",
       "2     5  25851     9.0\n",
       "3     6  25923     5.0\n",
       "4     7  25924     6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 11426)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test.user.unique()) - set(train.user.unique())), test.user.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSubmision(test_preds, name):\n",
    "    submition = pd.DataFrame()\n",
    "    submition['ID'] = test['ID']\n",
    "    submition['rating'] = test_preds\n",
    "    submition.to_csv(f'./data/submission_{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(number: float, min_round = 0.3, max_ronund = 0.7) -> float:\n",
    "    integer_part = int(number)\n",
    "    decimal_part = number - integer_part\n",
    "    \n",
    "    if decimal_part < 0.3:\n",
    "        return float(integer_part)  # Round down\n",
    "    elif decimal_part > 0.7:\n",
    "        return float(integer_part + 1)  # Round up\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>390351.000000</td>\n",
       "      <td>390351.000000</td>\n",
       "      <td>390351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35910.676834</td>\n",
       "      <td>68610.585878</td>\n",
       "      <td>7.604666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23425.777567</td>\n",
       "      <td>49826.877193</td>\n",
       "      <td>1.842793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13944.000000</td>\n",
       "      <td>30035.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35080.000000</td>\n",
       "      <td>52295.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55912.500000</td>\n",
       "      <td>104051.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77804.000000</td>\n",
       "      <td>185972.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user           item         rating\n",
       "count  390351.000000  390351.000000  390351.000000\n",
       "mean    35910.676834   68610.585878       7.604666\n",
       "std     23425.777567   49826.877193       1.842793\n",
       "min         1.000000       1.000000       1.000000\n",
       "25%     13944.000000   30035.000000       7.000000\n",
       "50%     35080.000000   52295.000000       8.000000\n",
       "75%     55912.500000  104051.000000       9.000000\n",
       "max     77804.000000  185972.000000      10.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43320.000000</td>\n",
       "      <td>43320.000000</td>\n",
       "      <td>43320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21659.500000</td>\n",
       "      <td>5335.888989</td>\n",
       "      <td>9626.943721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12505.551167</td>\n",
       "      <td>3376.304807</td>\n",
       "      <td>7673.878995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10829.750000</td>\n",
       "      <td>2283.750000</td>\n",
       "      <td>2470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21659.500000</td>\n",
       "      <td>4948.500000</td>\n",
       "      <td>8049.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32489.250000</td>\n",
       "      <td>8198.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43319.000000</td>\n",
       "      <td>11425.000000</td>\n",
       "      <td>25696.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID          user          item\n",
       "count  43320.000000  43320.000000  43320.000000\n",
       "mean   21659.500000   5335.888989   9626.943721\n",
       "std    12505.551167   3376.304807   7673.878995\n",
       "min        0.000000      0.000000      0.000000\n",
       "25%    10829.750000   2283.750000   2470.000000\n",
       "50%    21659.500000   4948.500000   8049.500000\n",
       "75%    32489.250000   8198.000000  15830.000000\n",
       "max    43319.000000  11425.000000  25696.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['rating'] = train['rating'].mean() * np.ones(test.shape[0])\n",
    "submition['ID'] = test['ID']\n",
    "submition.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = train.rating.mean()\n",
    "def predictByMeanUser(x):\n",
    "    if train[train.user == x].shape[0] > 0:\n",
    "        return train[train.user == x].rating.mean()\n",
    "    else:\n",
    "        return mean_train\n",
    "submition['rating'] = test.user.apply(predictByMeanUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']\n",
    "\n",
    "mean_train = train.rating.mean()\n",
    "def predictByMeanBook(x):\n",
    "    if train[train.item == x].shape[0] > 0:\n",
    "        return train[train.item == x].rating.mean()\n",
    "    else:\n",
    "        return mean_train\n",
    "submition['rating'] = test.item.apply(predictByMeanBook)\n",
    "\n",
    "submition.to_csv('./data/submission_meanbook.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']\n",
    "\n",
    "mean_train = train.rating.mean()\n",
    "def predictPonderedMean(x):\n",
    "    if train[train.item == x].shape[0] > 0:\n",
    "        book_mean =  train[train.item == x].rating.mean()\n",
    "    else:\n",
    "        book_mean = mean_train\n",
    "    if train[train.user == x].shape[0] > 0:\n",
    "        user_mean =  train[train.user == x].rating.mean()\n",
    "    else:\n",
    "        user_mean =  mean_train\n",
    "\n",
    "    return 0.4 * book_mean + 0.4 * user_mean + 0.2 * mean_train\n",
    "submition['rating'] = test.item.apply(predictByMeanBook)\n",
    "\n",
    "submition.to_csv('./data/submission_pondered_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization (1.509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100, verbose = 1):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        # Obtener usuarios y libros únicos\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        # Crear mapeos de IDs\n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        \n",
    "        # Inicializar matrices de factores latentes\n",
    "        self.user_factors = np.random.normal(0, 0.1, (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (len(self.items), self.n_factors))\n",
    "        \n",
    "        # Calcular el rating promedio global\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for _, row in ratings_df.iterrows():\n",
    "                user, item, rating = row['user'], row['item'], row['rating']\n",
    "                \n",
    "                # Obtener índices\n",
    "                user_idx = self.user_to_idx.get(user)\n",
    "                item_idx = self.item_to_idx.get(item)\n",
    "                \n",
    "                if user_idx is None or item_idx is None:\n",
    "                    continue\n",
    "                \n",
    "                # Calcular la predicción actual\n",
    "                pred = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "                \n",
    "                # Calcular el error\n",
    "                error = rating - pred\n",
    "                \n",
    "                # Actualizar factores\n",
    "                self.user_factors[user_idx] += self.learning_rate * (error * self.item_factors[item_idx] - self.regularization * self.user_factors[user_idx])\n",
    "                self.item_factors[item_idx] += self.learning_rate * (error * self.user_factors[user_idx] - self.regularization * self.item_factors[item_idx])\n",
    "            \n",
    "            # Opcionalmente, calcular el error del conjunto de entrenamiento\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                train_preds = self.predict(ratings_df)\n",
    "                rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            \n",
    "            user_idx = self.user_to_idx.get(user)\n",
    "            item_idx = self.item_to_idx.get(item)\n",
    "            \n",
    "            if user_idx is None or item_idx is None:\n",
    "                # Para nuevos usuarios o libros, usar el promedio global\n",
    "                predictions.append(self.global_mean)\n",
    "            else:\n",
    "                # Calcular la predicción\n",
    "                pred = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "                # Limitar la predicción al rango de ratings\n",
    "                pred = max(min(pred, 10.0), 1.0)  # Asumiendo ratings entre 1 y 10\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - RMSE: 1.6952\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "model = MatrixFactorization(n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=10)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "test_predictions = model.predict(test)\n",
    "generateSubmision(test_predictions, 'matrix_factorization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization 2 (1.292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "        self.user_to_idx = None\n",
    "        self.item_to_idx = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        \n",
    "    def create_mappings(self, ratings_df):\n",
    "        \"\"\"Crea mapeos de usuarios e ítems a índices\"\"\"\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        \n",
    "    # def create_matrix(self, df):\n",
    "    #     \"\"\"Crea una matriz dispersa a partir del DataFrame\"\"\"\n",
    "    #     rows = [self.user_to_idx.get(user) for user in df['user'] if user in self.user_to_idx]\n",
    "    #     cols = [self.item_to_idx.get(item) for item in df['item'] if item in self.item_to_idx]\n",
    "    #     ratings = df.loc[df['user'].isin(self.users) & df['item'].isin(self.items), 'rating'].values\n",
    "        \n",
    "    #     return csr_matrix((ratings, (rows, cols)), shape=(len(self.users), len(self.items)))\n",
    "    \n",
    "    def initialize_factors(self):\n",
    "        \"\"\"Inicializa las matrices de factores latentes y los sesgos\"\"\"\n",
    "        self.user_factors = np.random.normal(0, 0.1, (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (len(self.items), self.n_factors))\n",
    "        self.user_biases = np.zeros(len(self.users))\n",
    "        self.item_biases = np.zeros(len(self.items))\n",
    "        \n",
    "    def train_epoch(self, ratings_df):\n",
    "        \"\"\"Entrena el modelo por una época\"\"\"\n",
    "        # Convertir DataFrame a formato de coordenadas para acceso eficiente\n",
    "        users = ratings_df['user'].values\n",
    "        items = ratings_df['item'].values\n",
    "        ratings = ratings_df['rating'].values\n",
    "        \n",
    "        # Actualización de factores usando SGD (Stochastic Gradient Descent)\n",
    "        for i in range(len(ratings)):\n",
    "            user, item, rating = users[i], items[i], ratings[i]\n",
    "            \n",
    "            # Verificar si el usuario y el ítem existen en nuestros mapeos\n",
    "            if user not in self.user_to_idx or item not in self.item_to_idx:\n",
    "                continue\n",
    "                \n",
    "            user_idx = self.user_to_idx[user]\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            \n",
    "            # Calcular la predicción actual\n",
    "            pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "                   np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            \n",
    "            # Calcular el error\n",
    "            error = rating - pred\n",
    "            \n",
    "            # Actualizar sesgos\n",
    "            self.user_biases[user_idx] += self.learning_rate * (error - self.regularization * self.user_biases[user_idx])\n",
    "            self.item_biases[item_idx] += self.learning_rate * (error - self.regularization * self.item_biases[item_idx])\n",
    "            \n",
    "            # Actualizar factores\n",
    "            user_factor = self.user_factors[user_idx].copy()\n",
    "            item_factor = self.item_factors[item_idx].copy()\n",
    "            \n",
    "            self.user_factors[user_idx] += self.learning_rate * (error * item_factor - self.regularization * user_factor)\n",
    "            self.item_factors[item_idx] += self.learning_rate * (error * user_factor - self.regularization * item_factor)\n",
    "    \n",
    "    def fit(self, ratings_df, val_df=None, patience=20, verbose=True):\n",
    "        \"\"\"Entrena el modelo completo con early stopping opcional\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Crear mapeos y calcular el rating promedio global\n",
    "        self.create_mappings(ratings_df)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Inicializar factores latentes\n",
    "        self.initialize_factors()\n",
    "        \n",
    "        # Implementar early stopping si se proporciona un conjunto de validación\n",
    "        if val_df is not None:\n",
    "            best_val_rmse = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.n_epochs):\n",
    "                # Entrenar por una época\n",
    "                self.train_epoch(ratings_df)\n",
    "                \n",
    "                # Evaluar en el conjunto de validación\n",
    "                val_preds = self.predict(val_df)\n",
    "                val_rmse = np.sqrt(mean_squared_error(val_df['rating'], val_preds))\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Val RMSE: {val_rmse:.4f} - Tiempo: {elapsed:.2f}s\")\n",
    "                \n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    patience_counter = 0\n",
    "                    # Guardar el mejor modelo\n",
    "                    best_user_factors = self.user_factors.copy()\n",
    "                    best_item_factors = self.item_factors.copy()\n",
    "                    best_user_biases = self.user_biases.copy()\n",
    "                    best_item_biases = self.item_biases.copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping en la época {epoch+1} con {patience}\")\n",
    "                    # Restaurar el mejor modelo\n",
    "                    self.user_factors = best_user_factors\n",
    "                    self.item_factors = best_item_factors\n",
    "                    self.user_biases = best_user_biases\n",
    "                    self.item_biases = best_item_biases\n",
    "                    break\n",
    "        else:\n",
    "            # Sin early stopping, entrenar por un número fijo de épocas\n",
    "            for epoch in range(self.n_epochs):\n",
    "                self.train_epoch(ratings_df)\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    train_preds = self.predict(ratings_df)\n",
    "                    train_rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train RMSE: {train_rmse:.4f} - Tiempo: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Entrenamiento completado en {total_time:.2f} segundos\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_one(self, user, item):\n",
    "        \"\"\"Predice el rating para un par usuario-ítem específico (maneja cold start)\"\"\"\n",
    "        # Caso 1: Si el usuario y el ítem existen en nuestros mapeos\n",
    "        if user in self.user_to_idx and item in self.item_to_idx:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "                   np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            return max(min(pred, 10.0), 1.0)  # Limitar al rango [1, 10]\n",
    "        \n",
    "        # Caso 2: Si solo el usuario existe (nuevo ítem)\n",
    "        elif user in self.user_to_idx:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            return max(min(self.global_mean + self.user_biases[user_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Caso 3: Si solo el ítem existe (nuevo usuario)\n",
    "        elif item in self.item_to_idx:\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            return max(min(self.global_mean + self.item_biases[item_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Caso 4: Ni el usuario ni el ítem existen\n",
    "        else:\n",
    "            return self.global_mean\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        \"\"\"Predice ratings para un DataFrame de pares usuario-ítem\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            predictions.append(self.predict_one(user, item))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def guardar_modelo(modelo, filename):\n",
    "    \"\"\"Guarda el modelo en un archivo\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "\n",
    "def cargar_modelo(filename):\n",
    "    \"\"\"Carga el modelo desde un archivo\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 5/100 - Train RMSE: 1.5049 - Tiempo: 22.47s\n",
      "Época 10/100 - Train RMSE: 1.3694 - Tiempo: 77.91s\n",
      "Época 15/100 - Train RMSE: 1.2197 - Tiempo: 203.93s\n",
      "Época 20/100 - Train RMSE: 1.0639 - Tiempo: 363.62s\n",
      "Época 25/100 - Train RMSE: 0.9207 - Tiempo: 522.07s\n",
      "Época 30/100 - Train RMSE: 0.7989 - Tiempo: 586.39s\n",
      "Época 35/100 - Train RMSE: 0.6992 - Tiempo: 616.81s\n",
      "Época 40/100 - Train RMSE: 0.6190 - Tiempo: 645.52s\n",
      "Época 45/100 - Train RMSE: 0.5553 - Tiempo: 674.20s\n",
      "Época 50/100 - Train RMSE: 0.5048 - Tiempo: 704.37s\n",
      "Época 55/100 - Train RMSE: 0.4648 - Tiempo: 734.62s\n",
      "Época 60/100 - Train RMSE: 0.4330 - Tiempo: 762.96s\n",
      "Época 65/100 - Train RMSE: 0.4077 - Tiempo: 803.15s\n",
      "Época 70/100 - Train RMSE: 0.3875 - Tiempo: 872.24s\n",
      "Época 75/100 - Train RMSE: 0.3713 - Tiempo: 903.73s\n",
      "Época 80/100 - Train RMSE: 0.3582 - Tiempo: 935.40s\n",
      "Época 85/100 - Train RMSE: 0.3475 - Tiempo: 967.58s\n",
      "Época 90/100 - Train RMSE: 0.3387 - Tiempo: 999.83s\n",
      "Época 95/100 - Train RMSE: 0.3314 - Tiempo: 1031.54s\n",
      "Época 100/100 - Train RMSE: 0.3253 - Tiempo: 1063.60s\n",
      "Entrenamiento completado en 1073.23 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatrixFactorization at 0x2eb2e7fd1d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "modelo = MatrixFactorization(n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100)\n",
    "modelo.fit(train, val_df=None, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = modelo.predict(test)\n",
    "generateSubmision(test_preds, 'mf2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "guardar_modelo(modelo, 'matrixFact.pkl')\n",
    "print(\"Modelo guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds2 = [custom_round(x) for x in test_preds]\n",
    "generateSubmision(test_preds2, 'mf2_rounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = train.rating.mean()\n",
    "test_study = test.copy()\n",
    "test_study['rating'] = test_preds2\n",
    "test_study['decimal_rating'] = test_preds\n",
    "test_study['prediction_mean'] = test_study.rating.mean()\n",
    "test_study['prediction_mean'] = test_study.rating.mean()\n",
    "test_study['real_mean'] = global_mean\n",
    "test_study['item_mean'] = test_study.item.apply(predictByMeanBook)\n",
    "test_study['user_mean'] = test_study.item.apply(predictByMeanUser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF con matriz dispersa (1.292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "        self.user_to_idx = None\n",
    "        self.item_to_idx = None\n",
    "        self.idx_to_user = None\n",
    "        self.idx_to_item = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        \n",
    "    def create_mappings(self, ratings_df):\n",
    "        \"\"\"Creates mappings from users and items to indices\"\"\"\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        self.idx_to_user = {i: user for i, user in enumerate(self.users)}\n",
    "        self.idx_to_item = {i: item for i, item in enumerate(self.items)}\n",
    "        \n",
    "    def create_matrix(self, df):\n",
    "        \"\"\"Creates a sparse matrix from the DataFrame\"\"\"\n",
    "        # Filter out entries where user or item is not in our mappings\n",
    "        valid_df = df[df['user'].isin(self.users) & df['item'].isin(self.items)]\n",
    "        \n",
    "        rows = [self.user_to_idx[user] for user in valid_df['user']]\n",
    "        cols = [self.item_to_idx[item] for item in valid_df['item']]\n",
    "        ratings = valid_df['rating'].values\n",
    "        \n",
    "        return csr_matrix((ratings, (rows, cols)), shape=(len(self.users), len(self.items)))\n",
    "    \n",
    "    def initialize_factors(self):\n",
    "        \"\"\"Initializes latent factor matrices and biases\"\"\"\n",
    "        self.user_factors = np.random.normal(0, 0.1, (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (len(self.items), self.n_factors))\n",
    "        self.user_biases = np.zeros(len(self.users))\n",
    "        self.item_biases = np.zeros(len(self.items))\n",
    "        \n",
    "    def train_epoch(self, ratings_matrix):\n",
    "        \"\"\"Trains the model for one epoch using sparse matrix format\"\"\"\n",
    "        # Get non-zero elements from the sparse matrix (for faster iteration)\n",
    "        nonzero_indices = ratings_matrix.nonzero()\n",
    "        users_idx = nonzero_indices[0]\n",
    "        items_idx = nonzero_indices[1]\n",
    "        \n",
    "        # Shuffle the indices to ensure stochastic gradient descent\n",
    "        permutation = np.random.permutation(len(users_idx))\n",
    "        users_idx = users_idx[permutation]\n",
    "        items_idx = items_idx[permutation]\n",
    "        \n",
    "        # Loop through non-zero elements only for efficiency\n",
    "        for i in range(len(users_idx)):\n",
    "            user_idx = users_idx[i]\n",
    "            item_idx = items_idx[i]\n",
    "            \n",
    "            # Get the actual rating value from the sparse matrix\n",
    "            rating = ratings_matrix[user_idx, item_idx]\n",
    "            \n",
    "            # Calculate the current prediction\n",
    "            pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "                   np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            \n",
    "            # Calculate the error\n",
    "            error = rating - pred\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_biases[user_idx] += self.learning_rate * (error - self.regularization * self.user_biases[user_idx])\n",
    "            self.item_biases[item_idx] += self.learning_rate * (error - self.regularization * self.item_biases[item_idx])\n",
    "            \n",
    "            # Update factors\n",
    "            user_factor = self.user_factors[user_idx].copy()\n",
    "            item_factor = self.item_factors[item_idx].copy()\n",
    "            \n",
    "            self.user_factors[user_idx] += self.learning_rate * (error * item_factor - self.regularization * user_factor)\n",
    "            self.item_factors[item_idx] += self.learning_rate * (error * user_factor - self.regularization * item_factor)\n",
    "    \n",
    "    def fit(self, ratings_df, val_df=None, patience=20, verbose=True):\n",
    "        \"\"\"Trains the complete model with optional early stopping using sparse matrices\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create mappings and calculate global mean rating\n",
    "        self.create_mappings(ratings_df)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Convert dataframes to sparse matrices for more efficient training\n",
    "        train_matrix = self.create_matrix(ratings_df)\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.initialize_factors()\n",
    "        \n",
    "        # Implement early stopping if a validation set is provided\n",
    "        if val_df is not None:\n",
    "            val_matrix = self.create_matrix(val_df)\n",
    "            best_val_rmse = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.n_epochs):\n",
    "                # Train for one epoch\n",
    "                self.train_epoch(train_matrix)\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                val_preds = self.predict(val_df)\n",
    "                val_rmse = np.sqrt(mean_squared_error(val_df['rating'], val_preds))\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Epoch {epoch+1}/{self.n_epochs} - Val RMSE: {val_rmse:.4f} - Time: {elapsed:.2f}s\")\n",
    "                \n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    patience_counter = 0\n",
    "                    # Save the best model\n",
    "                    best_user_factors = self.user_factors.copy()\n",
    "                    best_item_factors = self.item_factors.copy()\n",
    "                    best_user_biases = self.user_biases.copy()\n",
    "                    best_item_biases = self.item_biases.copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1} with patience {patience}\")\n",
    "                    # Restore the best model\n",
    "                    self.user_factors = best_user_factors\n",
    "                    self.item_factors = best_item_factors\n",
    "                    self.user_biases = best_user_biases\n",
    "                    self.item_biases = best_item_biases\n",
    "                    break\n",
    "        else:\n",
    "            # No early stopping, train for a fixed number of epochs\n",
    "            for epoch in range(self.n_epochs):\n",
    "                self.train_epoch(train_matrix)\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    train_preds = self.predict(ratings_df)\n",
    "                    train_rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                    print(f\"Epoch {epoch+1}/{self.n_epochs} - Train RMSE: {train_rmse:.4f} - Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Training completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_one(self, user, item):\n",
    "        \"\"\"Predicts the rating for a specific user-item pair (handles cold start)\"\"\"\n",
    "        # Case 1: If both user and item exist in our mappings\n",
    "        if user in self.user_to_idx and item in self.item_to_idx:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "                   np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            return max(min(pred, 10.0), 1.0)  # Limit to range [1, 10]\n",
    "        \n",
    "        # Case 2: If only the user exists (new item)\n",
    "        elif user in self.user_to_idx:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            return max(min(self.global_mean + self.user_biases[user_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Case 3: If only the item exists (new user)\n",
    "        elif item in self.item_to_idx:\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            return max(min(self.global_mean + self.item_biases[item_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Case 4: Neither user nor item exists\n",
    "        else:\n",
    "            return self.global_mean\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        \"\"\"Predicts ratings for a DataFrame of user-item pairs\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            predictions.append(self.predict_one(user, item))\n",
    "        \n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train RMSE: 1.5833 - Time: 54.60s\n",
      "Epoch 10/100 - Train RMSE: 1.4771 - Time: 265.11s\n",
      "Epoch 15/100 - Train RMSE: 1.3819 - Time: 465.32s\n",
      "Epoch 20/100 - Train RMSE: 1.2842 - Time: 526.58s\n",
      "Epoch 25/100 - Train RMSE: 1.1823 - Time: 587.76s\n",
      "Epoch 30/100 - Train RMSE: 1.0814 - Time: 650.20s\n",
      "Epoch 35/100 - Train RMSE: 0.9850 - Time: 789.41s\n",
      "Epoch 40/100 - Train RMSE: 0.8957 - Time: 850.93s\n",
      "Epoch 45/100 - Train RMSE: 0.8143 - Time: 914.80s\n",
      "Epoch 50/100 - Train RMSE: 0.7408 - Time: 977.81s\n",
      "Epoch 55/100 - Train RMSE: 0.6748 - Time: 1040.55s\n",
      "Epoch 60/100 - Train RMSE: 0.6157 - Time: 1101.84s\n",
      "Epoch 65/100 - Train RMSE: 0.5628 - Time: 1163.19s\n",
      "Epoch 70/100 - Train RMSE: 0.5155 - Time: 1224.93s\n",
      "Epoch 75/100 - Train RMSE: 0.4732 - Time: 1286.81s\n",
      "Epoch 80/100 - Train RMSE: 0.4355 - Time: 1348.52s\n",
      "Epoch 85/100 - Train RMSE: 0.4017 - Time: 1409.62s\n",
      "Epoch 90/100 - Train RMSE: 0.3714 - Time: 1472.69s\n",
      "Epoch 95/100 - Train RMSE: 0.3442 - Time: 1535.80s\n",
      "Epoch 100/100 - Train RMSE: 0.3200 - Time: 1599.49s\n",
      "Training completed in 1609.66 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatrixFactorization at 0x2eb2e0d8950>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MatrixFactorization(n_factors=20, learning_rate=0.005, regularization=0.02, n_epochs=100)\n",
    "mf.fit(train, None, patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = mf.predict(test)\n",
    "generateSubmision(predictions, 'mf_sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar_modelo(modelo, 'sparseMAtrixFact.pkl')\n",
    "# print(\"Modelo guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMF (1.2562 - rounded -1.246)\n",
    "\n",
    "PMF (Probabilistic Matrix Factorization) descompone la matriz de calificaciones usuario-artículo en dos matrices de menor dimensión, representando los factores latentes de usuarios y artículos. A diferencia de la factorización matricial estándar, PMF utiliza un enfoque probabilístico, donde los factores latentes se asumen provenientes de distribuciones gaussianas. Las calificaciones observadas se modelan como generadas a partir de estos factores latentes más ruido gaussiano. El algoritmo usa estimación de Máxima A Posteriori (MAP), incorporando creencias previas sobre la distribución de factores. Esto permite al modelo manejar la incertidumbre en los datos, evitar sobreajustes y tratar los valores faltantes de manera natural, siendo especialmente útil para sistemas de recomendación con datos dispersos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class ProbabilisticMatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.005, user_regularization=0.1, item_regularization=0.1, n_epochs=100):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_regularization = user_regularization\n",
    "        self.item_regularization = item_regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "        self.user_to_idx = None\n",
    "        self.item_to_idx = None\n",
    "        self.idx_to_user = None\n",
    "        self.idx_to_item = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        # PMF typically doesn't use biases, but we can keep them for flexibility\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        # PMF parameters\n",
    "        self.rating_var = 0.1  # observation noise variance (1/precision)\n",
    "        self.user_prior_var = 1.0  # prior variance for user factors\n",
    "        self.item_prior_var = 1.0  # prior variance for item factors\n",
    "        \n",
    "    def create_mappings(self, ratings_df):\n",
    "        \"\"\"Creates mappings from users and items to indices\"\"\"\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        self.idx_to_user = {i: user for i, user in enumerate(self.users)}\n",
    "        self.idx_to_item = {i: item for i, item in enumerate(self.items)}\n",
    "        \n",
    "    def create_matrix(self, df):\n",
    "        \"\"\"Creates a sparse matrix from the DataFrame\"\"\"\n",
    "        # Filter out entries where user or item is not in our mappings\n",
    "        valid_df = df[df['user'].isin(self.users) & df['item'].isin(self.items)]\n",
    "        \n",
    "        rows = [self.user_to_idx[user] for user in valid_df['user']]\n",
    "        cols = [self.item_to_idx[item] for item in valid_df['item']]\n",
    "        ratings = valid_df['rating'].values\n",
    "        \n",
    "        return csr_matrix((ratings, (rows, cols)), shape=(len(self.users), len(self.items)))\n",
    "    \n",
    "    def initialize_factors(self):\n",
    "        \"\"\"Initializes latent factor matrices with Gaussian priors\"\"\"\n",
    "        # In PMF, factors are initialized from a Gaussian distribution\n",
    "        # with mean 0 and small variance\n",
    "        self.user_factors = np.random.normal(0, np.sqrt(self.user_prior_var/self.n_factors), \n",
    "                                            (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, np.sqrt(self.item_prior_var/self.n_factors), \n",
    "                                            (len(self.items), self.n_factors))\n",
    "        # Initialize biases to zero (PMF traditionally doesn't use biases)\n",
    "        self.user_biases = np.zeros(len(self.users))\n",
    "        self.item_biases = np.zeros(len(self.items))\n",
    "        \n",
    "    def train_epoch(self, ratings_matrix):\n",
    "        \"\"\"Trains the model for one epoch using MAP estimation\"\"\"\n",
    "        # Get non-zero elements from the sparse matrix (for faster iteration)\n",
    "        nonzero_indices = ratings_matrix.nonzero()\n",
    "        users_idx = nonzero_indices[0]\n",
    "        items_idx = nonzero_indices[1]\n",
    "        \n",
    "        # Shuffle the indices to ensure stochastic gradient descent\n",
    "        permutation = np.random.permutation(len(users_idx))\n",
    "        users_idx = users_idx[permutation]\n",
    "        items_idx = items_idx[permutation]\n",
    "        \n",
    "        # Scale regularization by precision (inverse variance)\n",
    "        user_reg_scaled = self.user_regularization / self.user_prior_var\n",
    "        item_reg_scaled = self.item_regularization / self.item_prior_var\n",
    "        \n",
    "        # Loop through non-zero elements only for efficiency\n",
    "        for i in range(len(users_idx)):\n",
    "            user_idx = users_idx[i]\n",
    "            item_idx = items_idx[i]\n",
    "            \n",
    "            # Get the actual rating value from the sparse matrix\n",
    "            rating = ratings_matrix[user_idx, item_idx]\n",
    "            \n",
    "            # Calculate the current prediction\n",
    "            pred = np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            # Add biases if using them (optional in PMF)\n",
    "            if self.user_biases is not None and self.item_biases is not None:\n",
    "                pred += self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx]\n",
    "            \n",
    "            # Calculate the error\n",
    "            error = rating - pred\n",
    "            \n",
    "            # Update biases if using them\n",
    "            if self.user_biases is not None and self.item_biases is not None:\n",
    "                self.user_biases[user_idx] += self.learning_rate * (error - user_reg_scaled * self.user_biases[user_idx])\n",
    "                self.item_biases[item_idx] += self.learning_rate * (error - item_reg_scaled * self.item_biases[item_idx])\n",
    "            \n",
    "            # Update factors using PMF update rules (MAP estimation)\n",
    "            # The derivatives come from the log posterior probability\n",
    "            user_factor = self.user_factors[user_idx].copy()\n",
    "            item_factor = self.item_factors[item_idx].copy()\n",
    "            \n",
    "            # PMF update rules (scaled by precision)\n",
    "            self.user_factors[user_idx] += self.learning_rate * ((error * item_factor) / self.rating_var - user_reg_scaled * user_factor)\n",
    "            self.item_factors[item_idx] += self.learning_rate * ((error * user_factor) / self.rating_var - item_reg_scaled * item_factor)\n",
    "    \n",
    "    def fit(self, ratings_df, val_df=None, patience=20, verbose=True):\n",
    "        \"\"\"Trains the complete model with optional early stopping using sparse matrices\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create mappings and calculate global mean rating\n",
    "        self.create_mappings(ratings_df)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Convert dataframes to sparse matrices for more efficient training\n",
    "        train_matrix = self.create_matrix(ratings_df)\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.initialize_factors()\n",
    "        \n",
    "        # Set priors based on data variance\n",
    "        # This is a common practice in PMF to set priors based on data characteristics\n",
    "        rating_std = ratings_df['rating'].std()\n",
    "        self.rating_var = rating_std ** 2  # Set observation noise variance\n",
    "        \n",
    "        # Implement early stopping if a validation set is provided\n",
    "        if val_df is not None:\n",
    "            val_matrix = self.create_matrix(val_df)\n",
    "            best_val_rmse = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.n_epochs):\n",
    "                # Train for one epoch\n",
    "                self.train_epoch(train_matrix)\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                val_preds = self.predict(val_df)\n",
    "                val_rmse = np.sqrt(mean_squared_error(val_df['rating'], val_preds))\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Epoch {epoch+1}/{self.n_epochs} - Val RMSE: {val_rmse:.4f} - Time: {elapsed:.2f}s\")\n",
    "                \n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    patience_counter = 0\n",
    "                    # Save the best model\n",
    "                    best_user_factors = self.user_factors.copy()\n",
    "                    best_item_factors = self.item_factors.copy()\n",
    "                    best_user_biases = self.user_biases.copy() if self.user_biases is not None else None\n",
    "                    best_item_biases = self.item_biases.copy() if self.item_biases is not None else None\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1} with patience {patience}\")\n",
    "                    # Restore the best model\n",
    "                    self.user_factors = best_user_factors\n",
    "                    self.item_factors = best_item_factors\n",
    "                    self.user_biases = best_user_biases\n",
    "                    self.item_biases = best_item_biases\n",
    "                    break\n",
    "        else:\n",
    "            # No early stopping, train for a fixed number of epochs\n",
    "            for epoch in range(self.n_epochs):\n",
    "                self.train_epoch(train_matrix)\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    train_preds = self.predict(ratings_df)\n",
    "                    train_rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                    print(f\"Epoch {epoch+1}/{self.n_epochs} - Train RMSE: {train_rmse:.4f} - Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Training completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_one(self, user, item):\n",
    "        \"\"\"Predicts the rating for a specific user-item pair\"\"\"\n",
    "        # Case 1: If both user and item exist in our mappings\n",
    "        if user in self.user_to_idx and item in self.item_to_idx:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            pred = np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            # Add biases if using them\n",
    "            if self.user_biases is not None and self.item_biases is not None:\n",
    "                pred += self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx]\n",
    "            return max(min(pred, 10.0), 1.0)  # Limit to range [1, 10]\n",
    "        \n",
    "        # Case 2: If only the user exists (new item)\n",
    "        elif user in self.user_to_idx and self.user_biases is not None:\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            return max(min(self.global_mean + self.user_biases[user_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Case 3: If only the item exists (new user)\n",
    "        elif item in self.item_to_idx and self.item_biases is not None:\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            return max(min(self.global_mean + self.item_biases[item_idx], 10.0), 1.0)\n",
    "        \n",
    "        # Case 4: Neither user nor item exists\n",
    "        else:\n",
    "            return self.global_mean\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        \"\"\"Predicts ratings for a DataFrame of user-item pairs\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            predictions.append(self.predict_one(user, item))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_log_likelihood(self, ratings_matrix):\n",
    "        \"\"\"Calculate log likelihood of the model given the data (for monitoring)\"\"\"\n",
    "        nonzero_indices = ratings_matrix.nonzero()\n",
    "        users_idx = nonzero_indices[0]\n",
    "        items_idx = nonzero_indices[1]\n",
    "        \n",
    "        log_likelihood = 0.0\n",
    "        \n",
    "        # Sum log likelihood of observed ratings\n",
    "        for i in range(len(users_idx)):\n",
    "            user_idx = users_idx[i]\n",
    "            item_idx = items_idx[i]\n",
    "            rating = ratings_matrix[user_idx, item_idx]\n",
    "            \n",
    "            pred = np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            if self.user_biases is not None and self.item_biases is not None:\n",
    "                pred += self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx]\n",
    "            \n",
    "            # Log likelihood for this rating (Gaussian likelihood)\n",
    "            log_lik = -0.5 * np.log(2 * np.pi * self.rating_var) - (0.5 / self.rating_var) * (rating - pred) ** 2\n",
    "            log_likelihood += log_lik\n",
    "        \n",
    "        # Add log prior for user factors\n",
    "        for u in range(len(self.users)):\n",
    "            log_prior_u = -0.5 * np.sum(self.user_factors[u] ** 2) / self.user_prior_var\n",
    "            log_likelihood += log_prior_u\n",
    "        \n",
    "        # Add log prior for item factors\n",
    "        for i in range(len(self.items)):\n",
    "            log_prior_i = -0.5 * np.sum(self.item_factors[i] ** 2) / self.item_prior_var\n",
    "            log_likelihood += log_prior_i\n",
    "        \n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Train RMSE: 1.2108 - Time: 48.44s\n",
      "Epoch 10/200 - Train RMSE: 1.0520 - Time: 100.95s\n",
      "Epoch 15/200 - Train RMSE: 0.9628 - Time: 164.38s\n",
      "Epoch 20/200 - Train RMSE: 0.9011 - Time: 225.54s\n",
      "Epoch 25/200 - Train RMSE: 0.8601 - Time: 280.31s\n",
      "Epoch 30/200 - Train RMSE: 0.8261 - Time: 334.11s\n",
      "Epoch 35/200 - Train RMSE: 0.8010 - Time: 387.81s\n",
      "Epoch 40/200 - Train RMSE: 0.7793 - Time: 440.90s\n",
      "Epoch 45/200 - Train RMSE: 0.7630 - Time: 488.80s\n",
      "Epoch 50/200 - Train RMSE: 0.7494 - Time: 545.09s\n",
      "Epoch 55/200 - Train RMSE: 0.7376 - Time: 599.40s\n",
      "Epoch 60/200 - Train RMSE: 0.7275 - Time: 660.34s\n",
      "Epoch 65/200 - Train RMSE: 0.7190 - Time: 803.43s\n",
      "Epoch 70/200 - Train RMSE: 0.7102 - Time: 850.83s\n",
      "Epoch 75/200 - Train RMSE: 0.7034 - Time: 895.47s\n",
      "Epoch 80/200 - Train RMSE: 0.6962 - Time: 940.19s\n",
      "Epoch 85/200 - Train RMSE: 0.6915 - Time: 984.28s\n",
      "Epoch 90/200 - Train RMSE: 0.6857 - Time: 1028.22s\n",
      "Epoch 95/200 - Train RMSE: 0.6823 - Time: 1070.95s\n",
      "Epoch 100/200 - Train RMSE: 0.6794 - Time: 1120.18s\n",
      "Epoch 105/200 - Train RMSE: 0.6748 - Time: 1163.74s\n",
      "Epoch 110/200 - Train RMSE: 0.6717 - Time: 1207.06s\n",
      "Epoch 115/200 - Train RMSE: 0.6685 - Time: 1251.58s\n",
      "Epoch 120/200 - Train RMSE: 0.6670 - Time: 1295.67s\n",
      "Epoch 125/200 - Train RMSE: 0.6637 - Time: 1339.71s\n",
      "Epoch 130/200 - Train RMSE: 0.6605 - Time: 1384.88s\n",
      "Epoch 135/200 - Train RMSE: 0.6589 - Time: 1429.45s\n",
      "Epoch 140/200 - Train RMSE: 0.6572 - Time: 1480.80s\n",
      "Epoch 145/200 - Train RMSE: 0.6549 - Time: 1540.76s\n",
      "Epoch 150/200 - Train RMSE: 0.6543 - Time: 1599.06s\n",
      "Epoch 155/200 - Train RMSE: 0.6505 - Time: 1659.28s\n",
      "Epoch 160/200 - Train RMSE: 0.6501 - Time: 1716.66s\n",
      "Epoch 165/200 - Train RMSE: 0.6488 - Time: 1774.53s\n",
      "Epoch 170/200 - Train RMSE: 0.6473 - Time: 1830.82s\n",
      "Epoch 175/200 - Train RMSE: 0.6473 - Time: 1887.94s\n",
      "Epoch 180/200 - Train RMSE: 0.6460 - Time: 1946.02s\n",
      "Epoch 185/200 - Train RMSE: 0.6455 - Time: 2005.11s\n",
      "Epoch 190/200 - Train RMSE: 0.6443 - Time: 2075.13s\n",
      "Epoch 195/200 - Train RMSE: 0.6419 - Time: 2147.30s\n",
      "Epoch 200/200 - Train RMSE: 0.6417 - Time: 2203.22s\n",
      "Training completed in 2211.93 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ProbabilisticMatrixFactorization at 0x1a4490fcb10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmf = ProbabilisticMatrixFactorization(n_factors=20, learning_rate=0.05, n_epochs=200)\n",
    "pmf.fit(train, None, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pmf.predict(test)\n",
    "generateSubmision(test_preds, 'pmf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_rounded = [custom_round(x) for x in test_preds]\n",
    "generateSubmision(test_preds_rounded, 'pmf_rounded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name  | Public score  |\n",
    "|---|---|\n",
    "| MF v1  | 1.509  |\n",
    "| MF sparse matrix  | 1.292  |\n",
    "| MF sparse matrix  custom_round| 1.273  |\n",
    "| PMF  |  1.262 |\n",
    "| PMF custom_round | 1.246  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
