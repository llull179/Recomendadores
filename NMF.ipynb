{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementacion del algoritmo Non-negative matrix factorization (NMF) en python para un sistema de recomendacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0zB2wqRcmpr"
      },
      "source": [
        "Cargamos los datos y la transformamos a sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m81r5Jg6cmpu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix,lil_matrix\n",
        "\n",
        "RANDOM_STATE = 46\n",
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBucwHOJVpwY"
      },
      "source": [
        "Observamos si existen usuarios o items que no esten en train pero si en test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Baz4166SEYxN",
        "outputId": "47b886e3-2f71-441d-fba2-0128ddc424f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero de usuarios en train: 73456\n",
            "Numero de usuarios en test: 11426\n",
            "Numero de usuarios en test que no estan en train: 4349\n",
            "Numero de items en train: 171171\n",
            "Numero de items en test: 25697\n",
            "Numero de items en test que no estan en train: 14802\n"
          ]
        }
      ],
      "source": [
        "user_train_set = set(train['user'].unique())\n",
        "user_test_set = set(test['user'].unique())\n",
        "print(f\"Numero de usuarios en train: {len(user_train_set)}\")\n",
        "print(f\"Numero de usuarios en test: {len(user_test_set)}\")\n",
        "non_intersecting_users = user_test_set - user_train_set\n",
        "print(f\"Numero de usuarios en test que no estan en train: {len(non_intersecting_users)}\")\n",
        "\n",
        "item_train_set = set(train['item'].unique())\n",
        "item_test_set = set(test['item'].unique())\n",
        "print(f\"Numero de items en train: {len(item_train_set)}\")\n",
        "print(f\"Numero de items en test: {len(item_test_set)}\")\n",
        "non_intersecting_items = item_test_set - item_train_set\n",
        "print(f\"Numero de items en test que no estan en train: {len(non_intersecting_items)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ6cLWfsVv6y"
      },
      "source": [
        "Podemos observar que en efecto existen 4349 usuarios y 14802 items que no estan en train, lo cual es un gran porcentaje de datos que el modelo no ha visto y va a tener que predecir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos una matriz sparse para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ilnZERLWcmpw"
      },
      "outputs": [],
      "source": [
        "# Find the number of users and items\n",
        "num_users = max(train[\"user\"].max(), test[\"user\"].max()) + 1\n",
        "num_items = max(train[\"item\"].max(), test[\"item\"].max()) + 1\n",
        "\n",
        "# Convert to CSR sparse matrices\n",
        "train_sparse = csr_matrix((train[\"rating\"], (train[\"user\"], train[\"item\"])), shape=(num_users, num_items))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhMtX85HX4QN"
      },
      "source": [
        "Definimos funciones de prediccion y error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dC4-LljXX7cu"
      },
      "outputs": [],
      "source": [
        "def predict_value(user, item, W, H):\n",
        "    \"\"\" Predict missing value for a given user-item pair \"\"\"\n",
        "    if user >= W.shape[0] or item >= H.shape[1]:\n",
        "        print(f\"Usuario con id {user} o item con id {item} no encontrado\")\n",
        "        return 0  # User or item not in training set\n",
        "\n",
        "    predict_value = np.dot(W[user], H[:, item])\n",
        "    return predict_value\n",
        "\n",
        "def get_mse_error(sparse_data, W, H):\n",
        "  mse_train_error = 0\n",
        "  num_vals = 0\n",
        "  rows, cols = sparse_data.nonzero()\n",
        "  for row, col in zip(rows, cols):\n",
        "    predicted_value = predict_value(row, col, W, H)\n",
        "    mse_train_error += (sparse_data[row, col] - predicted_value) ** 2\n",
        "    num_vals += 1\n",
        "\n",
        "  mse_train_error /= num_vals\n",
        "  return mse_train_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaeSR9iZX8r0"
      },
      "source": [
        "Implementacion de sklearn de NMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--0qAoiWfNRB",
        "outputId": "035875d7-a020-4d63-efe0-568b1e0a35c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "violation: 1.0\n",
            "violation: 0.1591180526649301\n",
            "violation: 0.10493736028066543\n",
            "violation: 0.07548202220587953\n",
            "violation: 0.055332187573304305\n",
            "violation: 0.03965358883135004\n",
            "violation: 0.02795975938731017\n",
            "violation: 0.019980271920264664\n",
            "violation: 0.01431331004288641\n",
            "violation: 0.010582078937317852\n",
            "violation: 0.008310907062502003\n",
            "violation: 0.0071086153530074084\n",
            "violation: 0.006372887975227188\n",
            "violation: 0.0057360354537136626\n",
            "violation: 0.0050158287423669205\n",
            "violation: 0.004346889238173937\n",
            "violation: 0.0038052606656561777\n",
            "violation: 0.003354606218958939\n",
            "violation: 0.0030016355004625515\n",
            "violation: 0.00270903622349938\n",
            "violation: 0.0024578466647988415\n",
            "violation: 0.002277902983319306\n",
            "violation: 0.002129690244748418\n",
            "violation: 0.0019883597408573168\n",
            "violation: 0.001878726900078003\n",
            "violation: 0.001796086058026549\n",
            "violation: 0.0017291201185336512\n",
            "violation: 0.0016705613936294304\n",
            "violation: 0.0016183829861253637\n",
            "violation: 0.001581044830815781\n",
            "violation: 0.0015263774457850796\n",
            "violation: 0.0014802843145249856\n",
            "violation: 0.001445921091032502\n",
            "violation: 0.0014135709973409568\n",
            "violation: 0.0013878371016997869\n",
            "violation: 0.0013709678315335042\n",
            "violation: 0.0013597883447267797\n",
            "violation: 0.00135512743234275\n",
            "violation: 0.0013565775610056173\n",
            "violation: 0.0013646854690273853\n",
            "violation: 0.001382524266143377\n",
            "violation: 0.0014079400047018371\n",
            "violation: 0.0014371866211931672\n",
            "violation: 0.0014678387278609997\n",
            "violation: 0.001499767723088368\n",
            "violation: 0.0015234165136500338\n",
            "violation: 0.0015448024932537918\n",
            "violation: 0.0015545660675225978\n",
            "violation: 0.0015585121936287011\n",
            "violation: 0.0015497853140380622\n",
            "violation: 0.0015286113987830181\n",
            "violation: 0.001492571942136994\n",
            "violation: 0.001449071640399186\n",
            "violation: 0.0014018233878464565\n",
            "violation: 0.001346775895247918\n",
            "violation: 0.0012829324943947281\n",
            "violation: 0.0012167717038161889\n",
            "violation: 0.0011542923327910156\n",
            "violation: 0.0010977910416547789\n",
            "violation: 0.0010472938084892653\n",
            "violation: 0.0010008779605044532\n",
            "violation: 0.0009576911462475882\n",
            "violation: 0.0009187088580634934\n",
            "violation: 0.0008832718891832333\n",
            "violation: 0.0008500937693149712\n",
            "violation: 0.0008200841387957975\n",
            "violation: 0.0007961685436349781\n",
            "violation: 0.0007754261568077194\n",
            "violation: 0.0007574026105232532\n",
            "violation: 0.0007425189566924629\n",
            "violation: 0.0007284852782087784\n",
            "violation: 0.0007172537178110465\n",
            "violation: 0.0007096018525975846\n",
            "violation: 0.0007027430075307609\n",
            "violation: 0.0006979896268920498\n",
            "violation: 0.00069415852638047\n",
            "violation: 0.000693376778232828\n",
            "violation: 0.0006945676171986224\n",
            "violation: 0.0006965541230374762\n",
            "violation: 0.0006977974502141938\n",
            "violation: 0.0006997430383731822\n",
            "violation: 0.0007018608295678761\n",
            "violation: 0.0007078450187595698\n",
            "violation: 0.0007136967269845419\n",
            "violation: 0.0007231381970428528\n",
            "violation: 0.0007356880702827081\n",
            "violation: 0.000752343552377872\n",
            "violation: 0.000773643904283976\n",
            "violation: 0.0007967124662110054\n",
            "violation: 0.0008228967021332144\n",
            "violation: 0.0008502173963095466\n",
            "violation: 0.0008791169037175437\n",
            "violation: 0.0009046479824720947\n",
            "violation: 0.0009227908177054248\n",
            "violation: 0.0009358665959162784\n",
            "violation: 0.000938177911723053\n",
            "violation: 0.0009293627913012018\n",
            "violation: 0.0009148427144469912\n",
            "violation: 0.0008906955270912757\n",
            "violation: 0.0008756275738225291\n",
            "violation: 0.0008674725322920232\n",
            "violation: 0.000874195820000754\n",
            "violation: 0.0008875672253789259\n",
            "violation: 0.0009092390068191578\n",
            "violation: 0.0009364142123958786\n",
            "violation: 0.0009682908087852082\n",
            "violation: 0.0010025604495734097\n",
            "violation: 0.0010415835094174115\n",
            "violation: 0.001081075178453919\n",
            "violation: 0.0011214927182696085\n",
            "violation: 0.0011543639224860318\n",
            "violation: 0.0011758803405398283\n",
            "violation: 0.0011729784857215351\n",
            "violation: 0.001138258698585699\n",
            "violation: 0.0011020255798084295\n",
            "violation: 0.0010748119610765638\n",
            "violation: 0.0010553111450606308\n",
            "violation: 0.0010348045060056578\n",
            "violation: 0.0010109832634735624\n",
            "violation: 0.0009804801649566112\n",
            "violation: 0.000937582530339022\n",
            "violation: 0.0008913367672886432\n",
            "violation: 0.0008434470868123407\n",
            "violation: 0.0007984222045053632\n",
            "violation: 0.0007555696389047329\n",
            "violation: 0.0007146871373664082\n",
            "violation: 0.0006732143468269276\n",
            "violation: 0.000635969031903081\n",
            "violation: 0.0006022858310552078\n",
            "violation: 0.0005707148565969141\n",
            "violation: 0.0005404162388337702\n",
            "violation: 0.0005108690360658666\n",
            "violation: 0.00048087387138693655\n",
            "violation: 0.0004522015075770522\n",
            "violation: 0.00042522142187869917\n",
            "violation: 0.0003999652195823194\n",
            "violation: 0.0003765024097016373\n",
            "violation: 0.0003545560327722199\n",
            "violation: 0.0003335966369574844\n",
            "violation: 0.00031407332439944074\n",
            "violation: 0.00029567348961766725\n",
            "violation: 0.00027793674498086747\n",
            "violation: 0.00026139821467596064\n",
            "violation: 0.0002458614883384499\n",
            "violation: 0.00023119776029674154\n",
            "violation: 0.00021762288423273103\n",
            "violation: 0.00020495980060990303\n",
            "violation: 0.0001930798006536554\n",
            "violation: 0.00018187262447541145\n",
            "violation: 0.0001714391698013424\n",
            "violation: 0.00016155270466427292\n",
            "violation: 0.00015227589292053073\n",
            "violation: 0.00014360569654798795\n",
            "violation: 0.00013539379453023583\n",
            "violation: 0.00012769241671111475\n",
            "violation: 0.00012045492716548719\n",
            "violation: 0.00011366191508956826\n",
            "violation: 0.00010729838976700292\n",
            "violation: 0.00010130585203818733\n",
            "violation: 9.563304792247551e-05\n",
            "Converged at iteration 161\n",
            "Reconstruction Error: 4561.42269616148\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import NMF\n",
        "num_components = 70\n",
        "model = NMF(n_components=num_components, random_state=RANDOM_STATE,alpha_W = 0.001,alpha_H = 0.001,max_iter = 400,verbose = False)\n",
        "W = model.fit_transform(train_sparse)\n",
        "H = model.components_\n",
        "\n",
        "print(\"Error de entrenamiento: \", get_mse_error(train_sparse, W, H))\n",
        "print(\"Error de validacion: \", get_mse_error(val_sparse, W, H))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HYKejIiY2Ya"
      },
      "source": [
        "Tras probar numerosas combinacion, sklearn no parece funcionar muy bien. De hecho la mayoria de valores que predice esta muy cercanos al 0. Una de las posibles razones puede ser el hecho de que trate los 0 como valores a predecir, en vez de valores nulos. Por ello vamos a realizar una implementacion manual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znLEeWPxnOrN"
      },
      "source": [
        "Esta implementacion contara ademas con un factor de regularizacion para evitar el sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sb8iH6VihMR9"
      },
      "outputs": [],
      "source": [
        "# Generamos nuestro propio NMF\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "\n",
        "def train_custom_nmf(X, X_val, num_users,num_items, k=10,l2_reg = 0, max_iter=100, tol=1e-4, verbose=True):\n",
        "    # Initialize W and H with small random values\n",
        "    W = np.abs(np.random.rand(num_users, k))\n",
        "    H = np.abs(np.random.rand(k, num_items))\n",
        "\n",
        "    # Convert to COO format for faster iteration over non-zero elements\n",
        "    X_coo = X.tocoo()\n",
        "\n",
        "    # Indices and data of non-zero elements\n",
        "    rows, cols, data = X_coo.row, X_coo.col, X_coo.data\n",
        "\n",
        "    # Previous error to check for convergence\n",
        "    prev_error = float('inf')\n",
        "\n",
        "    for iter_num in range(max_iter):\n",
        "        # Update W\n",
        "        for i in range(num_users):\n",
        "            # Get indices of non-zero elements in row i\n",
        "            indices = X_coo.row == i\n",
        "            if np.sum(indices) > 0:\n",
        "                j_indices = cols[indices]\n",
        "                x_values = data[indices]\n",
        "\n",
        "                h_subset = H[:, j_indices]\n",
        "\n",
        "                # Calculate numerator and denominator for the update rule\n",
        "                numerator = np.zeros(k)\n",
        "                denominator = np.zeros(k)\n",
        "\n",
        "                for idx, (j, x_val) in enumerate(zip(j_indices, x_values)):\n",
        "                    pred = np.dot(W[i], H[:, j])\n",
        "                    if pred > 0:  # Avoid division by zero\n",
        "                        numerator += x_val * H[:, j] / pred\n",
        "                        denominator += H[:, j]\n",
        "\n",
        "                # Regularization\n",
        "                l2_term = 2 * l2_reg * W[i]\n",
        "                denominator += l2_term\n",
        "\n",
        "                # Update rule\n",
        "                mask = denominator > 0\n",
        "                W[i, mask] *= numerator[mask] / (denominator[mask] + 1e-10)\n",
        "\n",
        "        # Update H\n",
        "        for j in range(num_items):\n",
        "            # Get indices of non-zero elements in column j\n",
        "            indices = X_coo.col == j\n",
        "            if np.sum(indices) > 0:\n",
        "                i_indices = rows[indices]\n",
        "                x_values = data[indices]\n",
        "\n",
        "                # Skip if no non-zero values\n",
        "                if len(x_values) == 0:\n",
        "                    continue\n",
        "\n",
        "                w_subset = W[i_indices, :]\n",
        "\n",
        "                # Calculate numerator and denominator for the update rule\n",
        "                numerator = np.zeros(k)\n",
        "                denominator = np.zeros(k)\n",
        "\n",
        "                for idx, (i, x_val) in enumerate(zip(i_indices, x_values)):\n",
        "                    pred = np.dot(W[i], H[:, j])\n",
        "                    if pred > 0:  # Avoid division by zero\n",
        "                        numerator += x_val * W[i] / pred\n",
        "                        denominator += W[i]\n",
        "\n",
        "                # Regularization\n",
        "                l2_term = 2 * l2_reg * H[:, j]\n",
        "                denominator += l2_term\n",
        "\n",
        "                # Update rule\n",
        "                mask = denominator > 0\n",
        "                H[mask, j] *= numerator[mask] / (denominator[mask] + 1e-10)\n",
        "\n",
        "\n",
        "        # Calculate error only on non-zero elements\n",
        "        error = get_mse_error(X, W, H)\n",
        "        if X_val is None:\n",
        "            val_error = 0\n",
        "        else:\n",
        "            val_error = get_mse_error(X_val, W, H)\n",
        "\n",
        "        # Check for convergence\n",
        "        improvement = prev_error - error\n",
        "        if verbose:\n",
        "            print(f\"Iteration {iter_num+1}: Train error = {error:.6f}, Validation error = {val_error:.6f}, Improvement = {improvement:.6f}\")\n",
        "\n",
        "        if 0 <= improvement < tol:\n",
        "            if verbose:\n",
        "                print(f\"Converged after {iter_num+1} iterations\")\n",
        "            break\n",
        "\n",
        "        prev_error = error\n",
        "\n",
        "    return W,H\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAwS5G-ZYUyb"
      },
      "source": [
        "Dividimos los datos en train y test, asegurandonos que train ha observado a los usuarios al menos una vez, este metodo no es perfecto (ya que es posible que haya items que train no haya visto), pero nos sirve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e2x4g5lwYo1G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "def train_test_split_sparse(ratings, test_percentage=0.2):\n",
        "    # Get the indices of all non-zero elements\n",
        "    nonzero_indices = ratings.nonzero()\n",
        "    user_interactions = {}\n",
        "\n",
        "    # Organize indices by user\n",
        "    for i in range(len(nonzero_indices[0])):\n",
        "        user = nonzero_indices[0][i]\n",
        "        item = nonzero_indices[1][i]\n",
        "        if user not in user_interactions:\n",
        "            user_interactions[user] = []\n",
        "        user_interactions[user].append(item)\n",
        "\n",
        "    # Create binary masks\n",
        "    train_mask = lil_matrix(ratings.shape, dtype=bool)\n",
        "    test_mask = lil_matrix(ratings.shape, dtype=bool)\n",
        "\n",
        "    # Split for each user\n",
        "    for user, items in user_interactions.items():\n",
        "        np.random.shuffle(items)\n",
        "        split_idx = max(1, int(len(items) * (1 - test_percentage)))  # Ensure at least one interaction remains\n",
        "        train_items, test_items = items[:split_idx], items[split_idx:]\n",
        "\n",
        "        for item in train_items:\n",
        "            train_mask[user, item] = True\n",
        "        for item in test_items:\n",
        "            test_mask[user, item] = True\n",
        "\n",
        "    # Apply masks to get train and test sets\n",
        "    train = ratings.multiply(train_mask)\n",
        "    test = ratings.multiply(test_mask)\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jhcTAzfYs6c"
      },
      "outputs": [],
      "source": [
        "train_sparse, val_sparse = train_test_split_sparse(train_sparse,test_percentage=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-u4y_WEl0ep",
        "outputId": "ff175ee8-0285-4887-8a04-1a97a8d2196b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: Train error = 1.745672, Validation error = 10.124449, Improvement = inf\n",
            "Iteration 2: Train error = 1.103722, Validation error = 7.532552, Improvement = 0.641950\n",
            "Iteration 3: Train error = 0.935341, Validation error = 7.163265, Improvement = 0.168382\n",
            "Iteration 4: Train error = 0.839340, Validation error = 6.998991, Improvement = 0.096001\n",
            "Iteration 5: Train error = 0.771041, Validation error = 6.898810, Improvement = 0.068298\n",
            "Iteration 6: Train error = 0.718186, Validation error = 6.828712, Improvement = 0.052855\n",
            "Iteration 7: Train error = 0.675421, Validation error = 6.776729, Improvement = 0.042765\n",
            "Iteration 8: Train error = 0.639804, Validation error = 6.737272, Improvement = 0.035617\n",
            "Iteration 9: Train error = 0.609505, Validation error = 6.707158, Improvement = 0.030299\n",
            "Iteration 10: Train error = 0.583305, Validation error = 6.684325, Improvement = 0.026201\n",
            "Error de entrenamiento:  0.5833048880318307\n",
            "Error de validacion:  6.684324843112412\n"
          ]
        }
      ],
      "source": [
        "W, H = train_custom_nmf(train_sparse,val_sparse, num_users,num_items, k=5 ,l2_reg = 0.001, max_iter=10, tol=1e-4, verbose=True)\n",
        "train_error = get_mse_error(train_sparse, W, H)\n",
        "print(\"Error de entrenamiento: \", train_error)\n",
        "\n",
        "if val_sparse is not None:\n",
        "  val_error = get_mse_error(val_sparse, W, H)\n",
        "  print(\"Error de validacion: \", val_error)\n",
        "\n",
        "else:\n",
        "  val_error = train_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-ozX9EmHSv"
      },
      "source": [
        "El dividir los datos en 2 conjuntos separados, parece que empeora los resultados, ya que se obtienen valores de validacion bastante bajos que son mmuy diferentes que el test de kaggle (valor de validacion de 6.7 vs 2.44 de test de kaggle).\n",
        "\n",
        "Esto puede ser debido a la mala division de datos, o a la falta de datos de train. Por ello vamos a evitar utilizar el conjunto de validacion y se va a usar el test de kaggle como forma de validacion para el NMF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LQThN_uiPL3T"
      },
      "outputs": [],
      "source": [
        "train_sparse = csr_matrix((train[\"rating\"], (train[\"user\"], train[\"item\"])), shape=(num_users, num_items))\n",
        "val_sparse = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgRbOEbemm1C"
      },
      "source": [
        "Generamos un conjunto de hiperparametros a probar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qQfEiIfgLks"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "param_grid = {\n",
        "    'k': [5,10],\n",
        "    'l2_reg': [0.001,0.1]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ujZiziYs1bCl",
        "outputId": "b6452fec-5d8c-4b4d-eb7e-3980f3016a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'k': 5, 'l2_reg': 0.001}\n",
            "Iteration 1: Train error = 1.790799, Validation error = 0.000000, Improvement = inf\n",
            "Iteration 2: Train error = 1.177148, Validation error = 0.000000, Improvement = 0.613651\n",
            "Iteration 3: Train error = 1.008289, Validation error = 0.000000, Improvement = 0.168860\n",
            "Iteration 4: Train error = 0.908586, Validation error = 0.000000, Improvement = 0.099702\n",
            "Iteration 5: Train error = 0.836957, Validation error = 0.000000, Improvement = 0.071629\n",
            "Iteration 6: Train error = 0.781400, Validation error = 0.000000, Improvement = 0.055558\n",
            "Iteration 7: Train error = 0.736447, Validation error = 0.000000, Improvement = 0.044953\n",
            "Iteration 8: Train error = 0.699027, Validation error = 0.000000, Improvement = 0.037419\n",
            "Iteration 9: Train error = 0.667217, Validation error = 0.000000, Improvement = 0.031811\n",
            "Iteration 10: Train error = 0.639723, Validation error = 0.000000, Improvement = 0.027494\n",
            "Error de entrenamiento:  0.6397229506191231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b03e252e988b>:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'k': 5, 'l2_reg': 0.1}\n",
            "Iteration 1: Train error = 1.754693, Validation error = 0.000000, Improvement = inf\n",
            "Iteration 2: Train error = 2.569946, Validation error = 0.000000, Improvement = -0.815252\n",
            "Iteration 3: Train error = 1.892533, Validation error = 0.000000, Improvement = 0.677412\n",
            "Iteration 4: Train error = 1.804586, Validation error = 0.000000, Improvement = 0.087947\n",
            "Iteration 5: Train error = 1.648169, Validation error = 0.000000, Improvement = 0.156417\n",
            "Iteration 6: Train error = 1.547540, Validation error = 0.000000, Improvement = 0.100629\n",
            "Iteration 7: Train error = 1.461848, Validation error = 0.000000, Improvement = 0.085691\n",
            "Iteration 8: Train error = 1.393262, Validation error = 0.000000, Improvement = 0.068586\n",
            "Iteration 9: Train error = 1.336082, Validation error = 0.000000, Improvement = 0.057180\n",
            "Iteration 10: Train error = 1.287981, Validation error = 0.000000, Improvement = 0.048101\n",
            "Error de entrenamiento:  1.2879805450241542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b03e252e988b>:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'k': 10, 'l2_reg': 0.001}\n",
            "Iteration 1: Train error = 1.316765, Validation error = 0.000000, Improvement = inf\n",
            "Iteration 2: Train error = 0.952668, Validation error = 0.000000, Improvement = 0.364097\n",
            "Iteration 3: Train error = 0.827960, Validation error = 0.000000, Improvement = 0.124709\n",
            "Iteration 4: Train error = 0.746358, Validation error = 0.000000, Improvement = 0.081602\n",
            "Iteration 5: Train error = 0.684128, Validation error = 0.000000, Improvement = 0.062230\n",
            "Iteration 6: Train error = 0.633846, Validation error = 0.000000, Improvement = 0.050282\n",
            "Iteration 7: Train error = 0.591923, Validation error = 0.000000, Improvement = 0.041923\n",
            "Iteration 8: Train error = 0.556229, Validation error = 0.000000, Improvement = 0.035694\n",
            "Iteration 9: Train error = 0.525359, Validation error = 0.000000, Improvement = 0.030870\n",
            "Iteration 10: Train error = 0.498327, Validation error = 0.000000, Improvement = 0.027032\n",
            "Error de entrenamiento:  0.49832655462221886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b03e252e988b>:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'k': 10, 'l2_reg': 0.1}\n",
            "Iteration 1: Train error = 1.455678, Validation error = 0.000000, Improvement = inf\n",
            "Iteration 2: Train error = 1.654947, Validation error = 0.000000, Improvement = -0.199270\n",
            "Iteration 3: Train error = 1.453508, Validation error = 0.000000, Improvement = 0.201439\n",
            "Iteration 4: Train error = 1.381044, Validation error = 0.000000, Improvement = 0.072464\n",
            "Iteration 5: Train error = 1.302005, Validation error = 0.000000, Improvement = 0.079038\n",
            "Iteration 6: Train error = 1.237705, Validation error = 0.000000, Improvement = 0.064300\n",
            "Iteration 7: Train error = 1.181786, Validation error = 0.000000, Improvement = 0.055919\n",
            "Iteration 8: Train error = 1.133223, Validation error = 0.000000, Improvement = 0.048563\n",
            "Iteration 9: Train error = 1.090445, Validation error = 0.000000, Improvement = 0.042778\n",
            "Iteration 10: Train error = 1.052411, Validation error = 0.000000, Improvement = 0.038035\n",
            "Error de entrenamiento:  1.0524107560011104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b03e252e988b>:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'k': 15, 'l2_reg': 0.001}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b03e252e988b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training with parameters: {param_combo}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_custom_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_combo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_combo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2_reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mse_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error de entrenamiento: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0213be8d71b8>\u001b[0m in \u001b[0;36mtrain_custom_nmf\u001b[0;34m(X, X_val, num_users, num_items, k, l2_reg, max_iter, tol, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Get indices of non-zero elements in row i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "#No usamos validacion\n",
        "val_sparse = None\n",
        "\n",
        "# Generate all combinations\n",
        "keys = param_grid.keys()\n",
        "values = param_grid.values()\n",
        "\n",
        "best_model = None\n",
        "best_error = float('inf')\n",
        "for combination in product(*values):\n",
        "    param_combo = dict(zip(keys, combination))\n",
        "    print(f\"Training with parameters: {param_combo}\")\n",
        "\n",
        "    W, H = train_custom_nmf(train_sparse,val_sparse, num_users,num_items, k=param_combo['k'],l2_reg = param_combo['l2_reg'], max_iter=10, tol=1e-4, verbose=True)\n",
        "    train_error = get_mse_error(train_sparse, W, H)\n",
        "    print(\"Error de entrenamiento: \", train_error)\n",
        "\n",
        "    if val_sparse is not None:\n",
        "      val_error = get_mse_error(val_sparse, W, H)\n",
        "      print(\"Error de validacion: \", val_error)\n",
        "\n",
        "    else:\n",
        "      val_error = train_error\n",
        "\n",
        "\n",
        "\n",
        "    # Realizamos las predicciones de test y las guardamos\n",
        "    submition = pd.DataFrame(columns=['ID', 'rating'])\n",
        "    for index, row in test.iterrows():\n",
        "        predicted_value = predict_value(row['user'], row['item'], W, H)\n",
        "        new_row = pd.DataFrame([{'ID': row['ID'], 'rating': predicted_value}])\n",
        "        submition = pd.concat([submition, new_row], ignore_index=True)\n",
        "\n",
        "    submition.to_csv(f'./submission_NMF_{param_combo}.csv', index=False)\n",
        "\n",
        "    # Guardamos el mejor modelo\n",
        "    if val_error < best_error:\n",
        "        best_error = val_error\n",
        "        best_model = (W, H)\n",
        "\n",
        "W = best_model[0]\n",
        "H = best_model[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Hiperparametros  | Resultados en kaggle|\n",
        "|------------|---------|\n",
        "| {'k': 5, 'l2_reg': 0.001}     |  2.414  |\n",
        "| {'k': 5, 'l2_reg': 0.1}     | 2.633 |\n",
        "| {'k': 10, 'l2_reg': 0.001}     | 2.072  |\n",
        "| {'k': 10, 'l2_reg': 0.1}     | 2.258 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hPJAyjiLOO"
      },
      "source": [
        "Los mejores parametros se obtienen con k = 10 y regularizacion muy pequeño de 0.001. Parece que incrementando k y disminuyendo la regularizacion, mejora bastante el algoritmo, probamos con mas iteraciones y k = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHRvbQQVjW6t",
        "outputId": "4994d87d-225a-4169-e89b-0fb744e2a6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: Train error = 1.017288, Validation error = 0.000000, Improvement = inf\n",
            "Iteration 2: Train error = 0.780894, Validation error = 0.000000, Improvement = 0.236394\n",
            "Iteration 3: Train error = 0.677252, Validation error = 0.000000, Improvement = 0.103642\n",
            "Iteration 4: Train error = 0.603905, Validation error = 0.000000, Improvement = 0.073347\n",
            "Iteration 5: Train error = 0.545884, Validation error = 0.000000, Improvement = 0.058021\n",
            "Iteration 6: Train error = 0.497994, Validation error = 0.000000, Improvement = 0.047890\n",
            "Iteration 7: Train error = 0.457529, Validation error = 0.000000, Improvement = 0.040464\n",
            "Iteration 8: Train error = 0.422787, Validation error = 0.000000, Improvement = 0.034742\n",
            "Iteration 9: Train error = 0.392587, Validation error = 0.000000, Improvement = 0.030200\n",
            "Iteration 10: Train error = 0.366069, Validation error = 0.000000, Improvement = 0.026518\n",
            "Iteration 11: Train error = 0.342586, Validation error = 0.000000, Improvement = 0.023483\n",
            "Iteration 12: Train error = 0.321639, Validation error = 0.000000, Improvement = 0.020947\n",
            "Iteration 13: Train error = 0.302837, Validation error = 0.000000, Improvement = 0.018802\n",
            "Iteration 14: Train error = 0.285868, Validation error = 0.000000, Improvement = 0.016969\n",
            "Iteration 15: Train error = 0.270480, Validation error = 0.000000, Improvement = 0.015388\n",
            "Error de entrenamiento:  0.27048010047572857\n"
          ]
        }
      ],
      "source": [
        "W, H = train_custom_nmf(train_sparse,val_sparse, num_users,num_items, k=30 ,l2_reg = 0.001, max_iter=15, tol=1e-4, verbose=True)\n",
        "train_error = get_mse_error(train_sparse, W, H)\n",
        "print(\"Error de entrenamiento: \", train_error)\n",
        "\n",
        "if val_sparse is not None:\n",
        "  val_error = get_mse_error(val_sparse, W, H)\n",
        "  print(\"Error de validacion: \", val_error)\n",
        "\n",
        "else:\n",
        "  val_error = train_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se obtiene un valor en kaggle de 1.526. Este valor mejora bastante en relacion al resto de implementaciones de NMF y seguramente pueda seguir mejorando, sin embargo no parece que se vaya a obtener la mejor solucion con esta implementacion, por tanto vamos a probar diferentes modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y84_j4-bcmp7"
      },
      "source": [
        "Codigo para realizar las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FPTfrUFcmp8",
        "outputId": "b75797e2-06d3-43f9-f591-90b45a658325"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-06eadbc14524>:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "submition = pd.DataFrame(columns=['ID', 'rating'])\n",
        "# Iterate row by row\n",
        "for index, row in test.iterrows():\n",
        "    predicted_value = predict_value(row['user'], row['item'], W, H)\n",
        "    new_row = pd.DataFrame([{'ID': row['ID'], 'rating': predicted_value}])\n",
        "    submition = pd.concat([submition, new_row], ignore_index=True)\n",
        "\n",
        "submition.to_csv('./submission_NMF_def.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOi6U96XUzf-"
      },
      "source": [
        "Dado que en test puede haber items y usuarios que no se encuentren en entrenamiento, sustituimos esos valores por la media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVPiSgb_U6k-",
        "outputId": "d99fe12d-591b-4564-9ee7-3db96e68244c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-1218c8665c7c>:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  submition = pd.concat([submition, new_row], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')\n",
        "user_train_set = set(train['user'].unique())\n",
        "item_train_set = set(train['item'].unique())\n",
        "\n",
        "submition = pd.DataFrame(columns=['ID', 'rating'])\n",
        "# Iterate row by row\n",
        "for index, row in test.iterrows():\n",
        "    if row['user'] not in user_train_set and row['item'] not in item_train_set:\n",
        "        predicted_value = train['rating'].mean()\n",
        "    elif row['user'] not in user_train_set:\n",
        "        predicted_value = train[train['item'] == row['item']]['rating'].mean()\n",
        "    elif row['item'] not in item_train_set:\n",
        "        predicted_value = train[train['user'] == row['user']]['rating'].mean()\n",
        "    else:\n",
        "        predicted_value = predict_value(row['user'], row['item'], W, H)\n",
        "\n",
        "    new_row = pd.DataFrame([{'ID': row['ID'], 'rating': predicted_value}])\n",
        "    submition = pd.concat([submition, new_row], ignore_index=True)\n",
        "\n",
        "submition.to_csv('./submission_NMF_def.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este metodo de reemplaza valores no vistos por la media mejora los resultados. Esto es debido a que el es muy dificil que el modelo pueda predecir datos de usuarios o items que nunca ha visto. Pasando de obtener un valor de 1.526 a 1.399."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
