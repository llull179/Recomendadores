{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Clase para gestionar el dataset\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, users, items, ratings=None):\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.ratings is not None:\n",
    "            return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "        else:\n",
    "            return self.users[idx], self.items[idx]\n",
    "\n",
    "# Modelo de factorización matricial con regularización\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=25, dropout_rate=0.3):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        \n",
    "        # Embeddings con menor dimensionalidad\n",
    "        self.user_embedding = nn.Embedding(num_users + 1, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim)\n",
    "        \n",
    "        # Capa completamente conectada más simple\n",
    "        self.fc = nn.Linear(embedding_dim * 2, 16)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # Obtener embeddings y aplicar dropout para regularización\n",
    "        user_emb = self.dropout(self.user_embedding(user))\n",
    "        item_emb = self.dropout(self.item_embedding(item))\n",
    "        \n",
    "        # Concatenar embeddings\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        \n",
    "        # Capa densa con regularización\n",
    "        x = F.leaky_relu(self.fc(x), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Tamaño del conjunto de entrenamiento: 390351\n",
      "Tamaño del conjunto de prueba: 43320\n",
      "Codificando usuarios e ítems...\n",
      "Creando dataloaders...\n",
      "Configurando modelo...\n",
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "print(\"Cargando datos...\")\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_data)}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(test_data)}\")\n",
    "\n",
    "# Codificar usuarios e ítems eficientemente\n",
    "print(\"Codificando usuarios e ítems...\")\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "train_users = torch.tensor(user_encoder.fit_transform(train_data['user']), dtype=torch.long)\n",
    "train_items = torch.tensor(item_encoder.fit_transform(train_data['item']), dtype=torch.long)\n",
    "train_ratings = torch.tensor(train_data['rating'].values, dtype=torch.float32)\n",
    "\n",
    "# Crear mapeo para usuarios e ítems\n",
    "user_mapping = {label: index for index, label in enumerate(user_encoder.classes_)}\n",
    "item_mapping = {label: index for index, label in enumerate(item_encoder.classes_)}\n",
    "\n",
    "# Función para manejar valores no vistos en el conjunto de prueba\n",
    "def encode_with_mapping(values, mapping):\n",
    "    return [mapping.get(val, len(mapping)) for val in values]\n",
    "\n",
    "# Preparar datos de prueba\n",
    "test_ids = torch.tensor(test_data['ID'].values, dtype=torch.long)\n",
    "test_users = torch.tensor(encode_with_mapping(test_data['user'], user_mapping), dtype=torch.long)\n",
    "test_items = torch.tensor(encode_with_mapping(test_data['item'], item_mapping), dtype=torch.long)\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "print(\"Creando dataloaders...\")\n",
    "train_dataset = RatingsDataset(train_users, train_items, train_ratings)\n",
    "test_dataset = RatingsDataset(test_users, test_items)\n",
    "\n",
    "batch_size = 1024  # Ajustar según memoria disponible\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "print(\"Configurando modelo...\")\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "embedding_dim = 32  # Reducir dimensionalidad para ahorrar memoria\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "model = MatrixFactorization(num_users, num_items, embedding_dim).to(device)\n",
    "\n",
    "# Configurar entrenamiento\n",
    "criterion = nn.MSELoss()\n",
    "weight_decay = 0.01  # Parámetro de regularización L2\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Época 1/100, Pérdida: 34.6033\n",
      "Época 2/100, Pérdida: 7.4611\n",
      "Época 3/100, Pérdida: 5.1246\n",
      "Época 4/100, Pérdida: 4.6920\n",
      "Época 5/100, Pérdida: 4.6247\n",
      "Época 6/100, Pérdida: 4.5707\n",
      "Época 7/100, Pérdida: 4.4922\n",
      "Época 8/100, Pérdida: 4.4114\n",
      "Época 9/100, Pérdida: 4.3106\n",
      "Época 10/100, Pérdida: 4.1784\n",
      "Época 11/100, Pérdida: 4.0656\n",
      "Época 12/100, Pérdida: 3.9673\n",
      "Época 13/100, Pérdida: 3.8816\n",
      "Época 14/100, Pérdida: 3.7894\n",
      "Época 15/100, Pérdida: 3.7229\n",
      "Época 16/100, Pérdida: 3.6461\n",
      "Época 17/100, Pérdida: 3.5704\n",
      "Época 18/100, Pérdida: 3.5035\n",
      "Época 19/100, Pérdida: 3.4375\n",
      "Época 20/100, Pérdida: 3.3478\n",
      "Época 21/100, Pérdida: 3.2803\n",
      "Época 22/100, Pérdida: 3.2096\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "num_epochs = 100\n",
    "early_stopping_patience = 10  # Paciencia para early stopping\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for user, item, rating in train_loader:\n",
    "        user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = criterion(prediction, rating)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping en época {epoch+1}\")\n",
    "        break\n",
    "    # Imprimir progreso\n",
    "    print(f\"Época {epoch+1}/{num_epochs}, Pérdida: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar predicciones para el conjunto de prueba\n",
    "def generate_test_predictions():\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "\n",
    "    # Conjuntos de usuarios e ítems conocidos en el conjunto de entrenamiento\n",
    "    known_users = set(train['user_id'].unique())\n",
    "    known_items = set(train['item_id'].unique())\n",
    "    global_mean = train['rating'].mean()  # Media global de ratings\n",
    "    with torch.no_grad():\n",
    "        for user, item in test_loader:\n",
    "            user, item = user.to(device), item.to(device)\n",
    "\n",
    "            # Convertir a listas para comprobar existencia en el conjunto de entrenamiento\n",
    "            user_list = user.cpu().numpy()\n",
    "            item_list = item.cpu().numpy()\n",
    "\n",
    "            predictions = []\n",
    "            for u, i in zip(user_list, item_list):\n",
    "                if u not in known_users or i not in known_items:\n",
    "                    predictions.append(global_mean)  # Usar media global\n",
    "                else:\n",
    "                    pred = model(torch.tensor([u], device=device), torch.tensor([i], device=device))\n",
    "                    predictions.append(torch.clamp(pred, min=1.0, max=10.0).item())\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # Crear dataframe con predicciones\n",
    "    results = pd.DataFrame({\n",
    "        'ID': test_ids.numpy(),\n",
    "        'rating': all_predictions\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluación con validación cruzada\n",
    "def evaluate_model():\n",
    "    # Usaremos una parte de los datos de entrenamiento como validación\n",
    "    val_size = int(0.1 * len(train_dataset))\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, [len(train_dataset) - val_size, val_size]\n",
    "    )\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in val_loader:\n",
    "            user, item, rating = user.to(device), item.to(device), rating.to(device)\n",
    "            pred = model(user, item)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            actuals.extend(rating.cpu().numpy())\n",
    "    \n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    return mse, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo...\n",
      "MSE: 1.2230\n",
      "RMSE: 1.1059\n",
      "MAE: 0.8395\n",
      "Generando predicciones...\n",
      "Primeras 5 predicciones:\n",
      "   ID    rating\n",
      "0   0  6.246171\n",
      "1   1  8.081882\n",
      "2   2  6.732179\n",
      "3   3  8.502872\n",
      "4   4  5.059886\n",
      "Predicciones guardadas en 'predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluación\n",
    "print(\"Evaluando modelo...\")\n",
    "mse, rmse, mae = evaluate_model()\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Generar predicciones\n",
    "print(\"Generando predicciones...\")\n",
    "predictions = generate_test_predictions()\n",
    "print(\"Primeras 5 predicciones:\")\n",
    "print(predictions.head())\n",
    "\n",
    "# Guardar predicciones\n",
    "predictions.to_csv('predictions.csv', index=False)\n",
    "print(\"Predicciones guardadas en 'predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
