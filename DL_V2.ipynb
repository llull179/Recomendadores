{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DEEP LEARNING"
      ],
      "metadata": {
        "id": "0N5i9GDH1XVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "ufd-wiDsugZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importa librerías necesarias y configura el entorno para entrenar un modelo de deep learning con PyTorch."
      ],
      "metadata": {
        "id": "--lDSHSM4vOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cargando datos...\")\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "train_users = torch.tensor(user_encoder.fit_transform(train_data['user']), dtype=torch.long)\n",
        "train_items = torch.tensor(item_encoder.fit_transform(train_data['item']), dtype=torch.long)\n",
        "train_ratings = torch.tensor(train_data['rating'].values, dtype=torch.float32)\n",
        "\n",
        "user_mapping = {label: index for index, label in enumerate(user_encoder.classes_)}\n",
        "item_mapping = {label: index for index, label in enumerate(item_encoder.classes_)}\n",
        "\n",
        "def encode_with_mapping(values, mapping):\n",
        "    return [mapping.get(val, len(mapping)) for val in values]\n",
        "\n",
        "test_ids = torch.tensor(test_data['ID'].values, dtype=torch.long)\n",
        "test_users = torch.tensor(encode_with_mapping(test_data['user'], user_mapping), dtype=torch.long)\n",
        "test_items = torch.tensor(encode_with_mapping(test_data['item'], item_mapping), dtype=torch.long)\n",
        "\n",
        "train_users_data, val_users_data, train_items_data, val_items_data, train_ratings_data, val_ratings_data = train_test_split(\n",
        "    train_users, train_items, train_ratings, test_size=0.05, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(train_users_data)}, Datos de validación: {len(val_users_data)}\")\n",
        "\n",
        "rating_mean = train_ratings_data.mean().item()\n",
        "rating_std = train_ratings_data.std().item()\n",
        "\n",
        "def normalize_ratings(ratings):\n",
        "    return (ratings - 1) / 9\n",
        "\n",
        "def denormalize_ratings(ratings):\n",
        "    return ratings * 9 + 1\n",
        "\n",
        "train_ratings_norm = normalize_ratings(train_ratings_data)\n",
        "val_ratings_norm = normalize_ratings(val_ratings_data)\n",
        "\n",
        "train_dataset = TensorDataset(train_users_data, train_items_data, train_ratings_norm)\n",
        "val_dataset = TensorDataset(val_users_data, val_items_data, val_ratings_norm)"
      ],
      "metadata": {
        "id": "q9K8oAoiugD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque de código carga y prepara datos para un sistema de recomendación. Codifica usuarios e ítems con LabelEncoder, normaliza las calificaciones a un rango de 0 a 1, divide el conjunto de datos en entrenamiento y validación, y convierte todo a tensores compatibles con PyTorch (TensorDataset)."
      ],
      "metadata": {
        "id": "jiuJ59WQ4iKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Recommender(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim, dropout_rate=0.3, leaky_relu_slope=0.2):\n",
        "        super(Recommender, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(embedding_dim * 2)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_embedding(user)\n",
        "        item_emb = self.item_embedding(item)\n",
        "\n",
        "        x = torch.cat([user_emb, item_emb], dim=1)\n",
        "        x = self.batch_norm1(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.leaky_relu(x)  # Replaced relu with leaky_relu\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.leaky_relu(x)  # Replaced relu with leaky_relu\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.leaky_relu(x)  # Replaced relu with leaky_relu\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x.squeeze()"
      ],
      "metadata": {
        "id": "janel-4Dxegw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo (Recommender) es una red neuronal que combina embeddings de usuarios e ítems, los procesa con múltiples capas densas, normalización y dropout, y produce una predicción entre 0 y 1 mediante una función sigmoid. Está diseñado para aprender relaciones latentes entre usuarios e ítems en un sistema de recomendación."
      ],
      "metadata": {
        "id": "mlvXnJ4c5x2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = len(user_encoder.classes_) + 1\n",
        "n_items = len(item_encoder.classes_) + 1\n",
        "embedding_dim = 64\n",
        "\n",
        "batch_size = 2048\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-5\n",
        "n_epochs = 100\n",
        "patience = 5\n",
        "\n",
        "model = Recommender(n_users, n_items, embedding_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "6lJFXb-4xj9i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se configuran los hiperparámetros del modelo, se define la arquitectura de la red neuronal (Recommender), y se preparan DataLoaders para el entrenamiento y validación. Se utiliza Adam como optimizador con regularización L2, y un scheduler para ajustar dinámicamente la tasa de aprendizaje cuando el rendimiento no mejora."
      ],
      "metadata": {
        "id": "hHLh_GYz6vlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for users, items, ratings in data_loader:\n",
        "            users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
        "            predictions = model(users, items)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            total_loss += loss.item() * len(ratings)\n",
        "    return total_loss / len(data_loader.dataset)"
      ],
      "metadata": {
        "id": "XyMx2ugA0Ypq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función evaluate mide el rendimiento del modelo calculando la pérdida promedio en un conjunto de datos sin actualizar parámetros. Utiliza MSELoss para comparar predicciones con valores reales y devuelve la pérdida media por muestra."
      ],
      "metadata": {
        "id": "mGcRgKxn65Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables para early stopping\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "best_model_path = 'best_recommender_model.pt'\n",
        "\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    train_bar = tqdm(train_loader, desc=f\"Época {epoch+1}/{n_epochs}\")\n",
        "\n",
        "    for users, items, ratings in train_bar:\n",
        "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(users, items)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * len(ratings)\n",
        "\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Época {epoch+1}/{n_epochs} | Tiempo: {elapsed_time:.2f}s | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Modelo guardado en {best_model_path}\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print(f\"EarlyStopping: {early_stop_counter}/{patience}\")\n",
        "\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"Early stopping activado después de {epoch+1} épocas\")\n",
        "        break\n",
        "\n",
        "print(f\"Entrenamiento completado en {time.time() - start_time:.2f} segundos\")"
      ],
      "metadata": {
        "id": "HiSrdpzA0lUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d3ad60-b3cb-4d7d-e18e-0c5108ca7065"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 1/100: 100%|██████████| 182/182 [00:36<00:00,  4.93it/s, loss=0.0368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1/100 | Tiempo: 38.62s | Train Loss: 0.039579 | Val Loss: 0.034157\n",
            "Modelo guardado en best_recommender_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 2/100: 100%|██████████| 182/182 [00:35<00:00,  5.15it/s, loss=0.0274]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 2/100 | Tiempo: 74.90s | Train Loss: 0.027693 | Val Loss: 0.035627\n",
            "EarlyStopping: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 3/100: 100%|██████████| 182/182 [00:36<00:00,  5.05it/s, loss=0.0129]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 3/100 | Tiempo: 111.43s | Train Loss: 0.019742 | Val Loss: 0.036185\n",
            "EarlyStopping: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 4/100: 100%|██████████| 182/182 [00:44<00:00,  4.12it/s, loss=0.0162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 4/100 | Tiempo: 156.03s | Train Loss: 0.015558 | Val Loss: 0.037612\n",
            "EarlyStopping: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 5/100: 100%|██████████| 182/182 [00:43<00:00,  4.22it/s, loss=0.0157]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 5/100 | Tiempo: 199.54s | Train Loss: 0.011911 | Val Loss: 0.037326\n",
            "EarlyStopping: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Época 6/100: 100%|██████████| 182/182 [00:37<00:00,  4.79it/s, loss=0.0111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 6/100 | Tiempo: 237.93s | Train Loss: 0.008762 | Val Loss: 0.038112\n",
            "EarlyStopping: 5/5\n",
            "Early stopping activado después de 6 épocas\n",
            "Entrenamiento completado en 237.93 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se entrena el modelo a través de múltiples épocas usando MSELoss como criterio de pérdida. Se implementa Early Stopping para detener el entrenamiento si la pérdida de validación no mejora tras varias épocas consecutivas. Además, se guarda el mejor modelo encontrado durante el entrenamiento."
      ],
      "metadata": {
        "id": "zIbtfU0v7ZPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cargando el mejor modelo para predicción...\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "print(\"Generando predicciones...\")\n",
        "predictions = []\n",
        "batch_size = 4096\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_users), batch_size):\n",
        "        batch_users = test_users[i:i+batch_size].to(device)\n",
        "        batch_items = test_items[i:i+batch_size].to(device)\n",
        "        batch_ids = test_ids[i:i+batch_size]\n",
        "\n",
        "        outputs = model(batch_users, batch_items)\n",
        "        predicted_ratings = (denormalize_ratings(outputs) * 10).round() / 10\n",
        "        predicted_ratings = predicted_ratings.clamp(1.0, 10.0)\n",
        "\n",
        "        predictions.extend(list(zip(batch_ids.cpu().numpy(), predicted_ratings.cpu().numpy())))\n",
        "\n",
        "predictions_df = pd.DataFrame(predictions, columns=['ID', 'rating'])\n",
        "predictions_df['rating'] = predictions_df['rating'].round(1)\n",
        "predictions_df.to_csv('submission-DL.csv', index=False)\n",
        "\n",
        "print('Predicciones guardadas en submission-DL.csv')"
      ],
      "metadata": {
        "id": "XE0wPg5c0nnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63681dc3-3e7d-474e-d243-c4c656ac9497"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando el mejor modelo para predicción...\n",
            "Generando predicciones...\n",
            "Predicciones guardadas en submission-DL.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se carga el mejor modelo guardado y se utiliza para predecir las calificaciones de un conjunto de prueba en lotes. Las predicciones se convierten a un formato adecuado y se exportan a un archivo CSV (submission-DL2.csv)."
      ],
      "metadata": {
        "id": "LdJYZ2Df7xOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONCLUSIÓN"
      ],
      "metadata": {
        "id": "m1cnmGm9TH2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque no fueron concebidas específicamente para sistemas de recomendación, las redes neuronales han demostrado ser herramientas excepcionalmente eficaces para esta actividad. Su versatilidad las convierte en una opción atractiva para abordar los desafíos de la recomendación, especialmente en conjuntos de datos grandes y complejos. Sin embargo, es importante reconocer que su implementación requiere un cuidadoso ajuste de hiperparámetros y entender bien el problema y como poder plantear la resolución del problema."
      ],
      "metadata": {
        "id": "tMzfY3CcTLUz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}