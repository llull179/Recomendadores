{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25715</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25716</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25851</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>25923</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>25924</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user   item  rating\n",
       "0     1  25715     7.0\n",
       "1     1  25716    10.0\n",
       "2     5  25851     9.0\n",
       "3     6  25923     5.0\n",
       "4     7  25924     6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 11426)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test.user.unique()) - set(train.user.unique())), test.user.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSubmision(test_preds, name):\n",
    "    submition = pd.DataFrame()\n",
    "    submition['ID'] = test['ID']\n",
    "    submition['rating'] = test_preds\n",
    "    submition.to_csv(f'./data/submission_{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19386</th>\n",
       "      <td>19386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  user  item\n",
       "19386  19386     0     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.item ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>390351.000000</td>\n",
       "      <td>390351.000000</td>\n",
       "      <td>390351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35910.676834</td>\n",
       "      <td>68610.585878</td>\n",
       "      <td>7.604666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23425.777567</td>\n",
       "      <td>49826.877193</td>\n",
       "      <td>1.842793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13944.000000</td>\n",
       "      <td>30035.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35080.000000</td>\n",
       "      <td>52295.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55912.500000</td>\n",
       "      <td>104051.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77804.000000</td>\n",
       "      <td>185972.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user           item         rating\n",
       "count  390351.000000  390351.000000  390351.000000\n",
       "mean    35910.676834   68610.585878       7.604666\n",
       "std     23425.777567   49826.877193       1.842793\n",
       "min         1.000000       1.000000       1.000000\n",
       "25%     13944.000000   30035.000000       7.000000\n",
       "50%     35080.000000   52295.000000       8.000000\n",
       "75%     55912.500000  104051.000000       9.000000\n",
       "max     77804.000000  185972.000000      10.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43320.000000</td>\n",
       "      <td>43320.000000</td>\n",
       "      <td>43320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21659.500000</td>\n",
       "      <td>5335.888989</td>\n",
       "      <td>9626.943721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12505.551167</td>\n",
       "      <td>3376.304807</td>\n",
       "      <td>7673.878995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10829.750000</td>\n",
       "      <td>2283.750000</td>\n",
       "      <td>2470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21659.500000</td>\n",
       "      <td>4948.500000</td>\n",
       "      <td>8049.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32489.250000</td>\n",
       "      <td>8198.000000</td>\n",
       "      <td>15830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43319.000000</td>\n",
       "      <td>11425.000000</td>\n",
       "      <td>25696.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID          user          item\n",
       "count  43320.000000  43320.000000  43320.000000\n",
       "mean   21659.500000   5335.888989   9626.943721\n",
       "std    12505.551167   3376.304807   7673.878995\n",
       "min        0.000000      0.000000      0.000000\n",
       "25%    10829.750000   2283.750000   2470.000000\n",
       "50%    21659.500000   4948.500000   8049.500000\n",
       "75%    32489.250000   8198.000000  15830.000000\n",
       "max    43319.000000  11425.000000  25696.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['rating'] = train['rating'].mean() * np.ones(test.shape[0])\n",
    "submition['ID'] = test['ID']\n",
    "submition.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = train.rating.mean()\n",
    "def predictByMeanUser(x):\n",
    "    if train[train.user == x].shape[0] > 0:\n",
    "        return train[train.user == x].rating.mean()\n",
    "    else:\n",
    "        return mean_train\n",
    "submition['rating'] = test.user.apply(predictByMeanUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']\n",
    "\n",
    "mean_train = train.rsating.mean()\n",
    "def predictByMeanBook(x):\n",
    "    if train[train.item == x].shape[0] > 0:\n",
    "        return train[train.item == x].rating.mean()\n",
    "    else:\n",
    "        return mean_train\n",
    "submition['rating'] = test.item.apply(predictByMeanBook)\n",
    "\n",
    "submition.to_csv('./data/submission_meanbook.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']\n",
    "\n",
    "mean_train = train.rating.mean()\n",
    "def predictPonderedMean(x):\n",
    "    if train[train.item == x].shape[0] > 0:\n",
    "        book_mean =  train[train.item == x].rating.mean()\n",
    "    else:\n",
    "        book_mean = mean_train\n",
    "    if train[train.user == x].shape[0] > 0:\n",
    "        user_mean =  train[train.user == x].rating.mean()\n",
    "    else:\n",
    "        user_mean =  mean_train\n",
    "\n",
    "    return 0.4 * book_mean + 0.4 * user_mean + 0.2 * mean_train\n",
    "submition['rating'] = test.item.apply(predictByMeanBook)\n",
    "\n",
    "submition.to_csv('./data/submission_pondered_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_14476\\3822933831.py\", line 1, in <module>\n",
      "    from surprise import Dataset, Reader, KNNBasic\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\__init__.py\", line 6, in <module>\n",
      "    from .prediction_algorithms import (\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\__init__.py\", line 23, in <module>\n",
      "    from .algo_base import AlgoBase\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\", line 8, in <module>\n",
      "    from .. import similarities as sims\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, KNNBasic\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\__init__.py:6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset_dir\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction_algorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     AlgoBase,\n\u001b[0;32m      8\u001b[0m     BaselineOnly,\n\u001b[0;32m      9\u001b[0m     CoClustering,\n\u001b[0;32m     10\u001b[0m     KNNBaseline,\n\u001b[0;32m     11\u001b[0m     KNNBasic,\n\u001b[0;32m     12\u001b[0m     KNNWithMeans,\n\u001b[0;32m     13\u001b[0m     KNNWithZScore,\n\u001b[0;32m     14\u001b[0m     NMF,\n\u001b[0;32m     15\u001b[0m     NormalPredictor,\n\u001b[0;32m     16\u001b[0m     Prediction,\n\u001b[0;32m     17\u001b[0m     PredictionImpossible,\n\u001b[0;32m     18\u001b[0m     SlopeOne,\n\u001b[0;32m     19\u001b[0m     SVD,\n\u001b[0;32m     20\u001b[0m     SVDpp,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainset\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`prediction_algorithms` package includes the prediction algorithms\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mavailable for recommendation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    co_clustering.CoClustering\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgo_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlgoBase\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline_only\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaselineOnly\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mco_clustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoClustering\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`surprise.prediction_algorithms.algo_base` module defines the base\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mclass :class:`AlgoBase` from which every single prediction algorithm has to\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03minherit.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m similarities \u001b[38;5;28;01mas\u001b[39;00m sims\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize_baselines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m baseline_als, baseline_sgd\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prediction, PredictionImpossible\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\similarities.pyx:1\u001b[0m, in \u001b[0;36minit surprise.similarities\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, KNNBasic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_14476\\2892930889.py\", line 2, in <module>\n",
      "    from surprise import Dataset, Reader, KNNBasic\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\__init__.py\", line 6, in <module>\n",
      "    from .prediction_algorithms import (\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\__init__.py\", line 23, in <module>\n",
      "    from .algo_base import AlgoBase\n",
      "  File \"c:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\", line 8, in <module>\n",
      "    from .. import similarities as sims\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, KNNBasic\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\__init__.py:6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset_dir\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction_algorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     AlgoBase,\n\u001b[0;32m      8\u001b[0m     BaselineOnly,\n\u001b[0;32m      9\u001b[0m     CoClustering,\n\u001b[0;32m     10\u001b[0m     KNNBaseline,\n\u001b[0;32m     11\u001b[0m     KNNBasic,\n\u001b[0;32m     12\u001b[0m     KNNWithMeans,\n\u001b[0;32m     13\u001b[0m     KNNWithZScore,\n\u001b[0;32m     14\u001b[0m     NMF,\n\u001b[0;32m     15\u001b[0m     NormalPredictor,\n\u001b[0;32m     16\u001b[0m     Prediction,\n\u001b[0;32m     17\u001b[0m     PredictionImpossible,\n\u001b[0;32m     18\u001b[0m     SlopeOne,\n\u001b[0;32m     19\u001b[0m     SVD,\n\u001b[0;32m     20\u001b[0m     SVDpp,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainset\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`prediction_algorithms` package includes the prediction algorithms\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mavailable for recommendation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    co_clustering.CoClustering\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgo_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlgoBase\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline_only\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaselineOnly\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mco_clustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoClustering\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`surprise.prediction_algorithms.algo_base` module defines the base\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mclass :class:`AlgoBase` from which every single prediction algorithm has to\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03minherit.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mheapq\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m similarities \u001b[38;5;28;01mas\u001b[39;00m sims\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize_baselines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m baseline_als, baseline_sgd\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prediction, PredictionImpossible\n",
      "File \u001b[1;32mc:\\Users\\Lluis\\anaconda3\\envs\\Master\\Lib\\site-packages\\surprise\\similarities.pyx:1\u001b[0m, in \u001b[0;36minit surprise.similarities\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class ProbabilisticMatrixFactorization(BaseEstimator):\n",
    "    def __init__(self, \n",
    "                 n_factors=10, \n",
    "                 learning_rate=0.01, \n",
    "                 reg_param=0.1, \n",
    "                 n_epochs=50,\n",
    "                 random_state=None):\n",
    "        # Hyperparameters\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.n_epochs = n_epochs\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Encoders and model state\n",
    "        self.user_encoder_ = LabelEncoder()\n",
    "        self.item_encoder_ = LabelEncoder()\n",
    "        \n",
    "        # Learned factors (to be set during fitting)\n",
    "        self.user_factors_ = None\n",
    "        self.item_factors_ = None\n",
    "        \n",
    "        # Training statistics\n",
    "        self.train_loss_curve_ = []\n",
    "    \n",
    "    def _encode_data(self, users, items):\n",
    "\n",
    "        user_indices = self.user_encoder_.fit_transform(users)\n",
    "        item_indices = self.item_encoder_.fit_transform(items)\n",
    "        return user_indices, item_indices\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Ensure consistent random state\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Encode users and items\n",
    "        user_indices, item_indices = self._encode_data(X[:, 0], X[:, 1])\n",
    "        \n",
    "        # Get number of unique users and items\n",
    "        n_users = len(self.user_encoder_.classes_)\n",
    "        n_items = len(self.item_encoder_.classes_)\n",
    "        \n",
    "        # Initialize user and item latent factor matrices\n",
    "        self.user_factors_ = np.random.normal(\n",
    "            0, 0.1, (n_users, self.n_factors)\n",
    "        )\n",
    "        self.item_factors_ = np.random.normal(\n",
    "            0, 0.1, (n_items, self.n_factors)\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Compute epoch loss\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Shuffle data for each epoch\n",
    "            shuffle_idx = np.random.permutation(len(user_indices))\n",
    "            user_indices = user_indices[shuffle_idx]\n",
    "            item_indices = item_indices[shuffle_idx]\n",
    "            ratings = y[shuffle_idx]\n",
    "            \n",
    "            # Iterate through all training samples\n",
    "            for u, i, r in zip(user_indices, item_indices, ratings):\n",
    "                # Predict rating\n",
    "                prediction = np.dot(\n",
    "                    self.user_factors_[u], \n",
    "                    self.item_factors_[i]\n",
    "                )\n",
    "                \n",
    "                # Compute error\n",
    "                error = r - prediction\n",
    "                epoch_loss += error ** 2\n",
    "                \n",
    "                # Compute gradients with regularization\n",
    "                # Update user factors\n",
    "                user_gradient = (\n",
    "                    error * self.item_factors_[i] - \n",
    "                    self.reg_param * self.user_factors_[u]\n",
    "                )\n",
    "                self.user_factors_[u] += self.learning_rate * user_gradient\n",
    "                \n",
    "                # Update item factors\n",
    "                item_gradient = (\n",
    "                    error * self.user_factors_[u] - \n",
    "                    self.reg_param * self.item_factors_[i]\n",
    "                )\n",
    "                self.item_factors_[i] += self.learning_rate * item_gradient\n",
    "            \n",
    "            # Store average epoch loss\n",
    "            self.train_loss_curve_.append(\n",
    "                np.sqrt(epoch_loss / len(user_indices))\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Encode users and items\n",
    "        user_indices = self.user_encoder_.transform(X[:, 0])\n",
    "        item_indices = self.item_encoder_.transform(X[:, 1])\n",
    "        \n",
    "        # Compute predictions\n",
    "        predictions = np.sum(\n",
    "            self.user_factors_[user_indices] * \n",
    "            self.item_factors_[item_indices], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def recommend_for_user(self, user, top_k=5):\n",
    "        # Encode user\n",
    "        try:\n",
    "            user_idx = self.user_encoder_.transform([user])[0]\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"User {user} not found in training data\")\n",
    "        \n",
    "        # Compute predicted ratings for all items\n",
    "        predicted_ratings = np.dot(\n",
    "            self.user_factors_[user_idx], \n",
    "            self.item_factors_.T\n",
    "        )\n",
    "        \n",
    "        # Get top K item indices\n",
    "        top_k_indices = predicted_ratings.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Convert back to original item identifiers\n",
    "        return self.item_encoder_.inverse_transform(top_k_indices)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        # Make predictions\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Compute metrics\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data(file_path, test_size=0.2, random_state=42):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Prepare data for model\n",
    "    X = df[['user', 'item']].values\n",
    "    y = df['rating'].values\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "X_train, y_train= load_and_prepare_data('./data/train.csv')\n",
    "\n",
    "# Initialize and train PMF model\n",
    "pmf = ProbabilisticMatrixFactorization(\n",
    "    n_factors=10,\n",
    "    learning_rate=0.01,\n",
    "    reg_param=0.1,\n",
    "    n_epochs=50,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_22800\\350037512.py:92: RuntimeWarning: overflow encountered in multiply\n",
      "  error * self.user_factors_[u] -\n",
      "C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_22800\\350037512.py:80: RuntimeWarning: overflow encountered in scalar power\n",
      "  epoch_loss += error ** 2\n",
      "C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_22800\\350037512.py:85: RuntimeWarning: overflow encountered in multiply\n",
      "  error * self.item_factors_[i] -\n",
      "C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_22800\\350037512.py:92: RuntimeWarning: invalid value encountered in subtract\n",
      "  error * self.user_factors_[u] -\n",
      "C:\\Users\\Lluis\\AppData\\Local\\Temp\\ipykernel_22800\\350037512.py:88: RuntimeWarning: invalid value encountered in add\n",
      "  self.user_factors_[u] += self.learning_rate * user_gradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate model performance\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 73\u001b[0m, in \u001b[0;36mProbabilisticMatrixFactorization.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Iterate through all training samples\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u, i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(user_indices, item_indices, ratings):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Predict rating\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_factors_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_factors_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Compute error\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     error \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m-\u001b[39m prediction\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "pmf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pmf.evaluate(X_train, y_train)\n",
    "print(\"Model Performance:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.upper()}: {value}\")\n",
    "\n",
    "# Example of recommending items for a user\n",
    "try:\n",
    "    # Assuming the first unique user in the dataset\n",
    "    sample_user = X_train[0, 0]\n",
    "    recommendations = pmf.recommend_for_user(sample_user, top_k=5)\n",
    "    \n",
    "    print(f\"\\nTop 5 Recommendations for User {sample_user}:\")\n",
    "    for item in recommendations:\n",
    "        print(item)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating recommendations: {e}\")\n",
    "\n",
    "# Optionally, plot training loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pmf.train_loss_curve_)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100, verbose = 1):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        # Obtener usuarios y libros únicos\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        # Crear mapeos de IDs\n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        \n",
    "        # Inicializar matrices de factores latentes\n",
    "        self.user_factors = np.random.normal(0, 0.1, (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (len(self.items), self.n_factors))\n",
    "        \n",
    "        # Calcular el rating promedio global\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for _, row in ratings_df.iterrows():\n",
    "                user, item, rating = row['user'], row['item'], row['rating']\n",
    "                \n",
    "                # Obtener índices\n",
    "                user_idx = self.user_to_idx.get(user)\n",
    "                item_idx = self.item_to_idx.get(item)\n",
    "                \n",
    "                if user_idx is None or item_idx is None:\n",
    "                    continue\n",
    "                \n",
    "                # Calcular la predicción actual\n",
    "                pred = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "                \n",
    "                # Calcular el error\n",
    "                error = rating - pred\n",
    "                \n",
    "                # Actualizar factores\n",
    "                self.user_factors[user_idx] += self.learning_rate * (error * self.item_factors[item_idx] - self.regularization * self.user_factors[user_idx])\n",
    "                self.item_factors[item_idx] += self.learning_rate * (error * self.user_factors[user_idx] - self.regularization * self.item_factors[item_idx])\n",
    "            \n",
    "            # Opcionalmente, calcular el error del conjunto de entrenamiento\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                train_preds = self.predict(ratings_df)\n",
    "                rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            \n",
    "            user_idx = self.user_to_idx.get(user)\n",
    "            item_idx = self.item_to_idx.get(item)\n",
    "            \n",
    "            if user_idx is None or item_idx is None:\n",
    "                # Para nuevos usuarios o libros, usar el promedio global\n",
    "                predictions.append(self.global_mean)\n",
    "            else:\n",
    "                # Calcular la predicción\n",
    "                pred = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "                # Limitar la predicción al rango de ratings\n",
    "                pred = max(min(pred, 10.0), 1.0)  # Asumiendo ratings entre 1 y 10\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_preds\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_preds' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.3958756638098913)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(train['rating'].mean() * np.ones(train.shape[0]),train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Crear y entrenar el modelo\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MatrixFactorization(n_factors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, regularization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 49\u001b[0m, in \u001b[0;36mMatrixFactorization.fit\u001b[1;34m(self, ratings_df)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     48\u001b[0m     train_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(ratings_df)\n\u001b[1;32m---> 49\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m(ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m], train_preds))\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "model = MatrixFactorization(n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=10)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "test_predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame()\n",
    "submition['ID'] = test['ID']\n",
    "submition['rating'] = test_predictions\n",
    "submition.to_csv('./data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390351"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43320, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "        self.user_to_idx = None\n",
    "        self.item_to_idx = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        self.user_biases = None\n",
    "        self.item_biases = None\n",
    "        \n",
    "    def create_mappings(self, ratings_df):\n",
    "        \"\"\"Crea mapeos de usuarios e ítems a índices\"\"\"\n",
    "        self.users = ratings_df['user'].unique()\n",
    "        self.items = ratings_df['item'].unique()\n",
    "        \n",
    "        self.user_to_idx = {user: i for i, user in enumerate(self.users)}\n",
    "        self.item_to_idx = {item: i for i, item in enumerate(self.items)}\n",
    "        \n",
    "    def create_matrix(self, df):\n",
    "        \"\"\"Crea una matriz dispersa a partir del DataFrame\"\"\"\n",
    "        rows = [self.user_to_idx.get(user) for user in df['user'] if user in self.user_to_idx]\n",
    "        cols = [self.item_to_idx.get(item) for item in df['item'] if item in self.item_to_idx]\n",
    "        ratings = df.loc[df['user'].isin(self.users) & df['item'].isin(self.items), 'rating'].values\n",
    "        \n",
    "        return csr_matrix((ratings, (rows, cols)), shape=(len(self.users), len(self.items)))\n",
    "    \n",
    "    def initialize_factors(self):\n",
    "        \"\"\"Inicializa las matrices de factores latentes y los sesgos\"\"\"\n",
    "        self.user_factors = np.random.normal(0, 0.1, (len(self.users), self.n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (len(self.items), self.n_factors))\n",
    "        self.user_biases = np.zeros(len(self.users))\n",
    "        self.item_biases = np.zeros(len(self.items))\n",
    "        \n",
    "    def train_epoch(self, ratings_df):\n",
    "        \"\"\"Entrena el modelo por una época\"\"\"\n",
    "        # Convertir DataFrame a formato de coordenadas para acceso eficiente\n",
    "        users = ratings_df['user'].values\n",
    "        items = ratings_df['item'].values\n",
    "        ratings = ratings_df['rating'].values\n",
    "        \n",
    "        # Actualización de factores usando SGD (Stochastic Gradient Descent)\n",
    "        for i in range(len(ratings)):\n",
    "            user, item, rating = users[i], items[i], ratings[i]\n",
    "            \n",
    "            # Verificar si el usuario y el ítem existen en nuestros mapeos\n",
    "            if user not in self.user_to_idx or item not in self.item_to_idx:\n",
    "                continue\n",
    "                \n",
    "            user_idx = self.user_to_idx[user]\n",
    "            item_idx = self.item_to_idx[item]\n",
    "            \n",
    "            # Calcular la predicción actual\n",
    "            pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "                   np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "            \n",
    "            # Calcular el error\n",
    "            error = rating - pred\n",
    "            \n",
    "            # Actualizar sesgos\n",
    "            self.user_biases[user_idx] += self.learning_rate * (error - self.regularization * self.user_biases[user_idx])\n",
    "            self.item_biases[item_idx] += self.learning_rate * (error - self.regularization * self.item_biases[item_idx])\n",
    "            \n",
    "            # Actualizar factores\n",
    "            user_factor = self.user_factors[user_idx].copy()\n",
    "            item_factor = self.item_factors[item_idx].copy()\n",
    "            \n",
    "            self.user_factors[user_idx] += self.learning_rate * (error * item_factor - self.regularization * user_factor)\n",
    "            self.item_factors[item_idx] += self.learning_rate * (error * user_factor - self.regularization * item_factor)\n",
    "    \n",
    "    def fit(self, ratings_df, val_df=None, patience=20, verbose=True):\n",
    "        \"\"\"Entrena el modelo completo con early stopping opcional\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Crear mapeos y calcular el rating promedio global\n",
    "        self.create_mappings(ratings_df)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "        \n",
    "        # Inicializar factores latentes\n",
    "        self.initialize_factors()\n",
    "        \n",
    "        # Implementar early stopping si se proporciona un conjunto de validación\n",
    "        if val_df is not None:\n",
    "            best_val_rmse = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.n_epochs):\n",
    "                # Entrenar por una época\n",
    "                self.train_epoch(ratings_df)\n",
    "                \n",
    "                # Evaluar en el conjunto de validación\n",
    "                val_preds = self.predict(val_df)\n",
    "                val_rmse = np.sqrt(mean_squared_error(val_df['rating'], val_preds))\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Val RMSE: {val_rmse:.4f} - Tiempo: {elapsed:.2f}s\")\n",
    "                \n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    patience_counter = 0\n",
    "                    # Guardar el mejor modelo\n",
    "                    best_user_factors = self.user_factors.copy()\n",
    "                    best_item_factors = self.item_factors.copy()\n",
    "                    best_user_biases = self.user_biases.copy()\n",
    "                    best_item_biases = self.item_biases.copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping en la época {epoch+1} con {patience}\")\n",
    "                    # Restaurar el mejor modelo\n",
    "                    self.user_factors = best_user_factors\n",
    "                    self.item_factors = best_item_factors\n",
    "                    self.user_biases = best_user_biases\n",
    "                    self.item_biases = best_item_biases\n",
    "                    break\n",
    "        else:\n",
    "            # Sin early stopping, entrenar por un número fijo de épocas\n",
    "            for epoch in range(self.n_epochs):\n",
    "                self.train_epoch(ratings_df)\n",
    "                \n",
    "                if verbose and (epoch + 1) % 5 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    train_preds = self.predict(ratings_df)\n",
    "                    train_rmse = np.sqrt(mean_squared_error(ratings_df['rating'], train_preds))\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train RMSE: {train_rmse:.4f} - Tiempo: {elapsed:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Entrenamiento completado en {total_time:.2f} segundos\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # def predict_one(self, user, item):\n",
    "    #     \"\"\"Predice el rating para un par usuario-ítem específico (maneja cold start)\"\"\"\n",
    "    #     # Caso 1: Si el usuario y el ítem existen en nuestros mapeos\n",
    "    #     if user in self.user_to_idx and item in self.item_to_idx:\n",
    "    #         user_idx = self.user_to_idx[user]\n",
    "    #         item_idx = self.item_to_idx[item]\n",
    "    #         pred = self.global_mean + self.user_biases[user_idx] + self.item_biases[item_idx] + \\\n",
    "    #                np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "    #         return max(min(pred, 10.0), 1.0)  # Limitar al rango [1, 10]\n",
    "        \n",
    "    #     # Caso 2: Si solo el usuario existe (nuevo ítem)\n",
    "    #     elif user in self.user_to_idx:\n",
    "    #         user_idx = self.user_to_idx[user]\n",
    "    #         return max(min(self.global_mean + self.user_biases[user_idx], 10.0), 1.0)\n",
    "        \n",
    "    #     # Caso 3: Si solo el ítem existe (nuevo usuario)\n",
    "    #     elif item in self.item_to_idx:\n",
    "    #         item_idx = self.item_to_idx[item]\n",
    "    #         return max(min(self.global_mean + self.item_biases[item_idx], 10.0), 1.0)\n",
    "        \n",
    "    #     # Caso 4: Ni el usuario ni el ítem existen\n",
    "    #     else:\n",
    "    #         return self.global_mean\n",
    "    \n",
    "    def predict(self, ratings_df):\n",
    "        \"\"\"Predice ratings para un DataFrame de pares usuario-ítem\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            predictions.append(self.predict_one(user, item))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def guardar_modelo(modelo, filename):\n",
    "    \"\"\"Guarda el modelo en un archivo\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "\n",
    "def cargar_modelo(filename):\n",
    "    \"\"\"Carga el modelo desde un archivo\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados correctamente.\n",
      "Tamaño del conjunto de entrenamiento: (390351, 3)\n",
      "Tamaño del conjunto de prueba: (43320, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(\"Datos cargados correctamente.\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {train_df.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {test_df.shape}\")\n",
    "\n",
    "# Crear conjunto de validación a partir del conjunto de entrenamento\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 5/100 - Val RMSE: 1.6552 - Tiempo: 19.92s\n",
      "Época 10/100 - Val RMSE: 1.6384 - Tiempo: 44.87s\n",
      "Época 15/100 - Val RMSE: 1.6350 - Tiempo: 76.02s\n",
      "Época 20/100 - Val RMSE: 1.6373 - Tiempo: 96.38s\n",
      "Época 25/100 - Val RMSE: 1.6425 - Tiempo: 118.49s\n",
      "Época 30/100 - Val RMSE: 1.6483 - Tiempo: 151.19s\n",
      "Época 35/100 - Val RMSE: 1.6536 - Tiempo: 172.49s\n",
      "Early stopping en la época 35 con 20\n",
      "Entrenamiento completado en 172.49 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatrixFactorization at 0x1ad9ca023d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "modelo = MatrixFactorization(n_factors=20, learning_rate=0.01, regularization=0.1, n_epochs=100)\n",
    "modelo.fit(train_df, val_df=val_df, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = modelo.predict(test_df)\n",
    "generateSubmision(test_preds, 'mf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_modelo(modelo, 'recomendador_libros.pkl')\n",
    "print(\"Modelo guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
